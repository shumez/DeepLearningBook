<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="None">
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>02. Linear Algebra - Deep Learning Book</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "02. Linear Algebra";
    var mkdocs_page_input_path = "02/index.md";
    var mkdocs_page_url = "/02/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Deep Learning Book</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../01/">01. Introduction</a>
	    </li>
          
            <li class="toctree-l1 current">
		
    <a class="current" href="./">02. Linear Algebra</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#contents">Contents</a></li>
    

    <li class="toctree-l2"><a href="#0200">02.00.</a></li>
    

    <li class="toctree-l2"><a href="#0201_scalars_vectors_matrices_and_tensors">02.01. Scalars, Vectors, Matrices and Tensors</a></li>
    

    <li class="toctree-l2"><a href="#0202_multiplying_matrices_and_vectors">02.02. Multiplying Matrices and Vectors</a></li>
    

    <li class="toctree-l2"><a href="#0203_identity_and_inverse_matrices">02.03. Identity and Inverse Matrices</a></li>
    

    <li class="toctree-l2"><a href="#0204_linear_dependence_and_span">02.04. Linear Dependence and Span</a></li>
    

    <li class="toctree-l2"><a href="#0205_norms">02.05. Norms</a></li>
    

    <li class="toctree-l2"><a href="#0206_special_kinds_of_matricesand_vectors">02.06. Special Kinds of Matricesand Vectors</a></li>
    

    <li class="toctree-l2"><a href="#0207_eigendecomposition">02.07. Eigendecomposition</a></li>
    

    <li class="toctree-l2"><a href="#0208_singular_value_decomposition">02.08. Singular Value Decomposition</a></li>
    

    <li class="toctree-l2"><a href="#0209_the_moore-penrose_pseudoinverse">02.09. The Moore-Penrose Pseudoinverse</a></li>
    

    <li class="toctree-l2"><a href="#0210_the_trace_operator">02.10. The Trace Operator</a></li>
    

    <li class="toctree-l2"><a href="#0211_the_determinant">02.11. The Determinant</a></li>
    

    <li class="toctree-l2"><a href="#0212_example_principal_components_analysis">02.12. Example: Principal Components Analysis</a></li>
    

    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Deep Learning Book</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>02. Linear Algebra</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <!--
Filename:   note.md
Project:    /Users/shume/Developer/DeepLearningBook/02
Author:     shumez <https://github.com/shumez>
Created:    2019-05-30 18:20:7
Modified:   2019-06-02 18:11:16
-----
Copyright (c) 2019 shumez
-->

<h1 id="02_linear_algebra"><a href="https://www.deeplearningbook.org/contents/linear_algebra.html">02. Linear Algebra</a><a class="headerlink" href="#02_linear_algebra" title="Permanent link">&para;</a></h1>
<h2 id="contents">Contents<a class="headerlink" href="#contents" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="#0200">02.00.</a></li>
<li><a href="#0201_scalars_vectors_matrices_and_tensors">02.01. Scalars, Vectors, Matrices and Tensors</a></li>
<li><a href="#0202_multiplying_matrices_and_vectors">02.02. Multiplying Matrices and Vectors</a></li>
<li><a href="#0203_identity_and_inverse_matrices">02.03. Identity and Inverse Matrices</a></li>
<li><a href="#0204_linear_dependence_and_span">02.04. Linear Dependence and Span</a></li>
<li><a href="#0205_norms">02.05. Norms</a></li>
<li><a href="#0206_special_kinds_of_matricesand_vectors">02.06. Special Kinds of Matricesand Vectors</a></li>
<li><a href="#0207_eigendecomposition">02.07. Eigendecomposition</a></li>
<li><a href="#0208_singular_value_decomposition">02.08. Singular Value Decomposition</a></li>
<li><a href="#0209_the_moore-penrose_pseudoinverse">02.09. The Moore-Penrose Pseudoinverse</a></li>
<li><a href="#0210_the_trace_operator">02.10. The Trace Operator</a></li>
<li><a href="#0211_the_determinant">02.11. The Determinant</a></li>
<li><a href="#0212_example_principal_components_analysis">02.12. Example: Principal Components Analysis</a></li>
</ul>
<h2 id="0200">02.00.<a class="headerlink" href="#0200" title="Permanent link">&para;</a></h2>
<p><em>The Matrix Cookbook</em> (<a href="#02_linear_algebra">Peterson, Pedersen, 2006</a>)<br />
<a href="#02_linear_algebra">Shilov, 1977</a></p>
<h2 id="0201_scalars_vectors_matrices_and_tensors">02.01. Scalars, Vectors, Matrices and Tensors<a class="headerlink" href="#0201_scalars_vectors_matrices_and_tensors" title="Permanent link">&para;</a></h2>
<ul>
<li>scalars</li>
<li>vectors: <script type="math/tex">\mathbf{x}</script>
</li>
</ul>
<p>
<script type="math/tex; mode=display">
    \mathbf{x} = 
    \begin{bmatrix}
        x_1 \\ x_2 \\ \vdots \\ x_n
    \end{bmatrix}
    \tag{2.1}
</script>
</p>
<ul>
<li>matrices: <script type="math/tex">\mathbf{A}</script>
</li>
</ul>
<p>
<script type="math/tex; mode=display">
    \mathbf{A} = 
    \begin{bmatrix}
        A_{1,1} & A_{1,2} \\
        A_{2,1} & A_{2,2} \\
        A_{3,1} & A_{3,2}
    \end{bmatrix} \Rightarrow \mathbf{A}^T =
    \begin{bmatrix}
        A_{1,1} & A_{2,1} & A_{3,1} \\
        A_{1,2} & A_{2,2} & A_{3,2}
    \end{bmatrix}
</script>
</p>
<ul>
<li>tensors: <script type="math/tex">\mathsf{A}</script>
</li>
</ul>
<p>
<script type="math/tex">A_{i, j, k}</script>
</p>
<p>def transposition
<script type="math/tex; mode=display"> (\mathbf{A}^T)_{i,j} = \mathbf{A}_{j,i} \tag{2.3} </script>
</p>
<p>
<script type="math/tex">\mathbf{C} = \mathbf{A} + \mathbf{B}</script> where <script type="math/tex">C_{i,j} = A_{i,j} + B_{i,j}</script>
</p>
<p>
<script type="math/tex">\mathbf{D} = a \cdot \mathbf{B} + c</script> where <script type="math/tex">D_{i,j} = a \cdot B_{i,j} + c</script>
</p>
<p>
<script type="math/tex">\mathbf{C} = \mathbf{A} + \mathbf{b}</script> where <script type="math/tex">C_{i,j} = A_{i,j} + b_j</script>
<br />
<strong>broadacsting</strong></p>
<h2 id="0202_multiplying_matrices_and_vectors">02.02. Multiplying Matrices and Vectors<a class="headerlink" href="#0202_multiplying_matrices_and_vectors" title="Permanent link">&para;</a></h2>
<p><strong>matrix product</strong><br />
<script type="math/tex; mode=display"> \mathbf{C} = \mathbf{AB} \tag{2.4} </script>
<script type="math/tex; mode=display"> C_{i,j} = \sum_k{A_{i,k} B_{k,j}} \tag{2.5} </script>
</p>
<p><strong>element-wise product</strong> (aka, <strong>Hadamard product</strong>)<br />
<script type="math/tex; mode=display"> \mathbf{C} = \mathbf{A} \odot \mathbf{B} </script>
<script type="math/tex; mode=display"> C_{i,j} = A_{i,j} B_{i,j} </script>
</p>
<p><strong>dot product</strong>: 
<script type="math/tex">\mathbf{x} \cdot \mathbf{y} = \mathbf{x}^T \mathbf{y}</script>
</p>
<p>
<script type="math/tex; mode=display"> \mathbf{x}^T \mathbf{y} = \mathbf{y}^T \mathbf{x} \tag{2.8} </script>
</p>
<p>
<script type="math/tex; mode=display"> (\mathbf{AB})^T = \mathbf{B}^T \mathbf{A}^T \tag{2.9} </script>
</p>
<p>
<script type="math/tex; mode=display"> \mathbf{x}^T \mathbf{y} = (\mathbf{x}^T \mathbf{y})^T = \mathbf{y}^T \mathbf{x} \tag{2.10} </script>
</p>
<p>a system of linear equations:</p>
<p>
<script type="math/tex; mode=display"> \mathbf{Ax} = \mathbf{b} \tag{2.11} </script>
</p>
<p>where <script type="math/tex">\mathbf{A} \in \mathbb{R}^{m \times n}</script>, <script type="math/tex">\mathbf{b} \in \mathbb{R}^m</script>, <script type="math/tex">\mathbf{x} \in \mathbb{R}^n</script>
</p>
<p>
<script type="math/tex; mode=display">
    \begin{align*}
        \mathbf{A}_{1,:} \mathbf{x} &= b_1 \tag{2.12} \\
        \mathbf{A}_{2,:} \mathbf{x} &= b_2 \tag{2.13} \\
        \vdots \tag{2.14} \\
        \mathbf{A}_{m,:} \mathbf{x} &= b_m \tag{2.15} \\
    \end{align*}
</script>
</p>
<p>
<script type="math/tex; mode=display"> 
    \begin{align*}
        \mathbf{A}_{1,1} x_1 + \mathbf{A}_{1,2} x_2 + \cdots + \mathbf{A}_{1,n} x_n &= b_1 \tag{2.16} \\
        \mathbf{A}_{2,1} x_1 + \mathbf{A}_{2,2} x_2 + \cdots + \mathbf{A}_{2,n} x_n &= b_2 \tag{2.17} \\
        \vdots \tag{2.18} \\
        \mathbf{A}_{m,1} x_1 + \mathbf{A}_{m,2} x_2 + \cdots + \mathbf{A}_{m,n} x_n &= b_m \tag{2.19} 
    \end{align*}
</script>
</p>
<h2 id="0203_identity_and_inverse_matrices">02.03. Identity and Inverse Matrices<a class="headerlink" href="#0203_identity_and_inverse_matrices" title="Permanent link">&para;</a></h2>
<p><strong>matrix inversion</strong></p>
<p><strong>identity matrix</strong> <script type="math/tex">\mathbf{I}_n \in \mathbb{R}^{n \times n}</script>
</p>
<p>
<script type="math/tex; mode=display"> \forall\mathbf{x} \in \mathbb{R}^n, \mathbf{I}_n \mathbf{x} = \mathbf{x} \tag{2.20} </script>
</p>
<p>
<script type="math/tex; mode=display"> \mathbf{A}^{-1} \mathbf{A} = \mathbf{I}_n \tag{2.21} </script>
</p>
<h2 id="0204_linear_dependence_and_span">02.04. Linear Dependence and Span<a class="headerlink" href="#0204_linear_dependence_and_span" title="Permanent link">&para;</a></h2>
<p>in order for <script type="math/tex">\mathbf{A}^{-1}</script> to exist</p>
<p>
<script type="math/tex; mode=display"> \mathbf{Ax} = \mathbf{b} \tag{2.11} </script>
</p>
<p>eq.2.11 must have one solution</p>
<p>possible to have no solution / infinitely many solutions<br />
<strong>NOT</strong> possible to have more than 1 but less than infinitely solutions</p>
<p>if both <script type="math/tex">\mathbf{x}</script> and <script type="math/tex">\mathbf{y}</script> are solutions then</p>
<p>
<script type="math/tex; mode=display"> \mathbf{z} = \alpha \mathbf{x} + (1 - \alpha) \mathbf{y} \tag{2.26} </script>
</p>
<p>is also solution for any real <script type="math/tex">\alpha</script>
</p>
<p>
<script type="math/tex; mode=display"> \mathbf{Ax} = \sum_i{x_i \mathbf{A}_{:,i}} \tag{2.27} </script>
</p>
<p><strong>linear combination</strong></p>
<p>
<script type="math/tex">\{ \mathbf{v}^{(1)}, \cdots, \mathbf{v}^{(n)} \}</script>
</p>
<p>
<script type="math/tex; mode=display"> \sum_i{c_i \mathbf{v}^{(i)}} \tag{2.28} </script>
</p>
<p><strong>span</strong> </p>
<p>
<script type="math/tex">\mathbf{Ax} = \mathbf{b}</script> have solution for all vals of <script type="math/tex">\mathbf{b} \in \mathbb{R}^m</script>
</p>
<h2 id="0205_norms">02.05. Norms<a class="headerlink" href="#0205_norms" title="Permanent link">&para;</a></h2>
<p><strong>norm</strong>: fn that measure size of vectors </p>
<p>
<script type="math/tex">L^p</script> norm is given by</p>
<p>
<script type="math/tex; mode=display"> ||x||_p = \Bigg( \sum_i{|x_i|^p} \Bigg)^{\frac{1}{p}} \tag{2.30} </script>
</p>
<p>for <script type="math/tex">p \in \mathbb{R}</script>, <script type="math/tex">p \ge 1</script>
</p>
<ul>
<li>
<script type="math/tex">f(\mathbf{x}) = 0</script> &rArr; <script type="math/tex">x=0</script>
</li>
<li>
<script type="math/tex">f(\mathbf{x} + \mathbf{y}) \le f(\mathbf{x}) + f(\mathbf{y})</script> (<strong>triangle inequality</strong>)</li>
<li>
<script type="math/tex">\forall\alpha \in \mathbb{R}</script>, <script type="math/tex">f(\alpha \mathbf{x}) = |\alpha| f(x)</script>
</li>
</ul>
<p>
<script type="math/tex">L^2</script> norm, with <script type="math/tex">p=2</script> ka <strong>Euclidean norm</strong> (<script type="math/tex">||x||</script>)</p>
<p>
<script type="math/tex">L^1</script> norm</p>
<p>
<script type="math/tex; mode=display"> ||x||_1 = \sum_i{|x_i|} \tag{2.31} </script>
</p>
<p>
<script type="math/tex">L^\infty</script> norm (aka, <strong>max norm</strong>)</p>
<p>
<script type="math/tex; mode=display"> ||x||_\infty = \max_i{|x_i|} \tag{2.32} </script>
</p>
<p><strong>Frobenius norm</strong></p>
<p>
<script type="math/tex; mode=display"> ||A||_F = \sqrt{\sum_{i,j}{\mathbf{A}_{i,j}^2}} \tag{2.33} </script>
is analogous to <script type="math/tex">L^2</script> norm of vec</p>
<p>
<script type="math/tex; mode=display"> \mathbf{x}^T \mathbf{y} = ||\mathbf{x}||_2 ||\mathbf{y}||_2 \cos{\theta} \tag{2.34} </script>
</p>
<p>where <script type="math/tex">\theta</script> is angle between <script type="math/tex">\mathbf{x}</script>, <script type="math/tex">\mathbf{y}</script>
</p>
<h2 id="0206_special_kinds_of_matricesand_vectors">02.06. Special Kinds of Matricesand Vectors<a class="headerlink" href="#0206_special_kinds_of_matricesand_vectors" title="Permanent link">&para;</a></h2>
<p><strong>diagonal</strong> <script type="math/tex">\mathbf{D}</script>
<br />
if and only if <script type="math/tex">D_{i,j}=0</script> for all <script type="math/tex">i \ne j</script>
</p>
<pre><code class="py">np.random.seed(123)
v = np.random.randint(9, size=5)
np.diag(v)
</code></pre>

<pre><code class="py">array([[2, 0, 0, 0, 0],
       [0, 2, 0, 0, 0],
       [0, 0, 6, 0, 0],
       [0, 0, 0, 1, 0],
       [0, 0, 0, 0, 3]])
</code></pre>

<p>
<script type="math/tex; mode=display"> \text{diag}(\mathbf{v}) \mathbf{x} = \mathbf{v} \odot \mathbf{x} </script>
</p>
<pre><code class="py">np.matmul(np.diag(v), x)

np.multiply(v, x)
</code></pre>

<p>
<script type="math/tex; mode=display"> \text{diag}(\mathbf{v})^{-1} = \text{diag}\Bigg( \bigg[\frac{1}{v_1}, \cdots \frac{1}{v_n} \bigg]^T \Bigg) </script>
</p>
<pre><code class="py">np.linalg.inv(diag_v)
np.diag([1/vi for vi in v])
</code></pre>

<p><strong>symmetric</strong> mat</p>
<p>
<script type="math/tex; mode=display"> \mathbf{A} = \mathbf{A}^T \tag{2.35} </script>
</p>
<p><strong>unit vector</strong>: vec w/ <strong>unit norm</strong>:</p>
<p>
<script type="math/tex; mode=display"> ||x||_2 = 1 \tag{2.36} </script>
</p>
<p>vec <script type="math/tex">\mathbf{x}</script> and vec <script type="math/tex">\mathbf{y}</script>
<strong>orthogonal</strong> to each other if <script type="math/tex">\mathbf{x}^T \mathbf{y} = 0</script>
</p>
<p>orthogonal &amp; unit norm <strong>orthonormal</strong></p>
<p><strong>orthogonal matrix</strong></p>
<p>
<script type="math/tex; mode=display"> \mathbf{A}^T \mathbf{A} = \mathbf{A}\mathbf{A}^T = \mathbf{I} \tag{2.37} </script>
</p>
<p>implyies 
<script type="math/tex; mode=display"> \mathbf{A}^{-1} = \mathbf{A}^T \tag{2.38} </script>
</p>
<pre><code class="py">from scipy.stats import ortho_group

A = ortho_group.rvs(dim = 3)

np.set_printoptions(suppress=True)
AT_A = np.matmul(A, A.T)
</code></pre>

<h2 id="0207_eigendecomposition">02.07. Eigendecomposition<a class="headerlink" href="#0207_eigendecomposition" title="Permanent link">&para;</a></h2>
<p><strong>eigendecomposition</strong>: into set of eigenvectors &amp; eigenvalues</p>
<p><strong>eigenvector</strong> of square mat <script type="math/tex">\mathbf{A}</script> is non-zero vec <script type="math/tex">\mathbf{v}</script>
<script type="math/tex; mode=display"> \mathbf{A}\mathbf{v} = \lambda \mathbf{v} \tag{2.39} </script>
</p>
<p><strong>eigenvalue</strong> <script type="math/tex">\lambda</script>
</p>
<pre><code class="py">l, V = np.linalg.eig(A)
</code></pre>

<p>mat <script type="math/tex">\mathbf{A}</script> has <script type="math/tex">n</script> linearly independent eigenvec <script type="math/tex">\{ \mathbf{v}^{(1)}, \cdots, \mathbf{v}^{(n)} \}</script>,
w/ corresponding eigenval <script type="math/tex">\{ \lambda_1, \cdots, \lambda_n \}</script>
</p>
<p>mat <script type="math/tex">\mathbf{V}</script>: <script type="math/tex">\mathbf{V} = [ \mathbf{v}^{(1)}, \cdots, \mathbf{v}^{(b)} ] </script>,<br />
vec <script type="math/tex">\lambda</script>: <script type="math/tex">[\lambda_1, \cdots, \lambda_n]^T</script>
</p>
<p><strong>eigendecomposition</strong></p>
<p>
<script type="math/tex; mode=display"> \mathbf{A} = \mathbf{V} \text{diag}(\lambda) \mathbf{V}^{-1} \tag{2.40} </script>
</p>
<h2 id="0208_singular_value_decomposition">02.08. Singular Value Decomposition<a class="headerlink" href="#0208_singular_value_decomposition" title="Permanent link">&para;</a></h2>
<h2 id="0209_the_moore-penrose_pseudoinverse">02.09. The Moore-Penrose Pseudoinverse<a class="headerlink" href="#0209_the_moore-penrose_pseudoinverse" title="Permanent link">&para;</a></h2>
<h2 id="0210_the_trace_operator">02.10. The Trace Operator<a class="headerlink" href="#0210_the_trace_operator" title="Permanent link">&para;</a></h2>
<h2 id="0211_the_determinant">02.11. The Determinant<a class="headerlink" href="#0211_the_determinant" title="Permanent link">&para;</a></h2>
<h2 id="0212_example_principal_components_analysis">02.12. Example: Principal Components Analysis<a class="headerlink" href="#0212_example_principal_components_analysis" title="Permanent link">&para;</a></h2>
<h2 id="_1"><a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<!-- toc -->

<!-- ref -->

<!-- fig -->

<!-- term -->

<style type="text/css">
    img{width: 51%; float: right;}
</style>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="../01/" class="btn btn-neutral" title="01. Introduction"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
      <p>Copyright &copy; 2018 - 2019 <a href="https://github.com/shumez">shumez</a>
</p>
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../01/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>

<!--
MkDocs version : 1.0.2
Build Date UTC : 2019-06-02 09:32:38
-->

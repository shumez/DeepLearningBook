<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="None">
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>02. Linear Algebra - Deep Learning Book</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "02. Linear Algebra";
    var mkdocs_page_input_path = "02/index.md";
    var mkdocs_page_url = "/02/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Deep Learning Book</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../01/">01. Introduction</a>
	    </li>
          
            <li class="toctree-l1 current">
		
    <a class="current" href="./">02. Linear Algebra</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#contents">Contents</a></li>
    

    <li class="toctree-l2"><a href="#0200">02.00.</a></li>
    

    <li class="toctree-l2"><a href="#0201_scalars_vectors_matrices_and_tensors">02.01. Scalars, Vectors, Matrices and Tensors</a></li>
    

    <li class="toctree-l2"><a href="#0202_multiplying_matrices_and_vectors">02.02. Multiplying Matrices and Vectors</a></li>
    

    <li class="toctree-l2"><a href="#0203_identity_and_inverse_matrices">02.03. Identity and Inverse Matrices</a></li>
    

    <li class="toctree-l2"><a href="#0204_linear_dependence_and_span">02.04. Linear Dependence and Span</a></li>
    

    <li class="toctree-l2"><a href="#0205_norms">02.05. Norms</a></li>
    

    <li class="toctree-l2"><a href="#0206_special_kinds_of_matricesand_vectors">02.06. Special Kinds of Matricesand Vectors</a></li>
    

    <li class="toctree-l2"><a href="#0207_eigendecomposition">02.07. Eigendecomposition</a></li>
    

    <li class="toctree-l2"><a href="#0208_singular_value_decomposition">02.08. Singular Value Decomposition</a></li>
    

    <li class="toctree-l2"><a href="#0209_the_moore-penrose_pseudoinverse">02.09. The Moore-Penrose Pseudoinverse</a></li>
    

    <li class="toctree-l2"><a href="#0210_the_trace_operator">02.10. The Trace Operator</a></li>
    

    <li class="toctree-l2"><a href="#0211_the_determinant">02.11. The Determinant</a></li>
    

    <li class="toctree-l2"><a href="#0212_example_principal_components_analysis">02.12. Example: Principal Components Analysis</a></li>
    

    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../03/">03. Probability and Information Theory</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Deep Learning Book</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>02. Linear Algebra</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <!--
Filename:   note.md
Project:    /Users/shume/Developer/DeepLearningBook/02
Author:     shumez <https://github.com/shumez>
Created:    2019-05-30 18:20:7
Modified:   2019-06-03 20:55:1
-----
Copyright (c) 2019 shumez
-->

<h1 id="02_linear_algebra"><a href="https://www.deeplearningbook.org/contents/linear_algebra.html">02. Linear Algebra</a><a class="headerlink" href="#02_linear_algebra" title="Permanent link">&para;</a></h1>
<h2 id="contents">Contents<a class="headerlink" href="#contents" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="#0200">02.00.</a></li>
<li><a href="#0201_scalars_vectors_matrices_and_tensors">02.01. Scalars, Vectors, Matrices and Tensors</a></li>
<li><a href="#0202_multiplying_matrices_and_vectors">02.02. Multiplying Matrices and Vectors</a></li>
<li><a href="#0203_identity_and_inverse_matrices">02.03. Identity and Inverse Matrices</a></li>
<li><a href="#0204_linear_dependence_and_span">02.04. Linear Dependence and Span</a></li>
<li><a href="#0205_norms">02.05. Norms</a></li>
<li><a href="#0206_special_kinds_of_matricesand_vectors">02.06. Special Kinds of Matricesand Vectors</a></li>
<li><a href="#0207_eigendecomposition">02.07. Eigendecomposition</a></li>
<li><a href="#0208_singular_value_decomposition">02.08. Singular Value Decomposition</a></li>
<li><a href="#0209_the_moore-penrose_pseudoinverse">02.09. The Moore-Penrose Pseudoinverse</a></li>
<li><a href="#0210_the_trace_operator">02.10. The Trace Operator</a></li>
<li><a href="#0211_the_determinant">02.11. The Determinant</a></li>
<li><a href="#0212_example_principal_components_analysis">02.12. Example: Principal Components Analysis</a></li>
</ul>
<h2 id="0200">02.00.<a class="headerlink" href="#0200" title="Permanent link">&para;</a></h2>
<p><em>The Matrix Cookbook</em> (<a href="#02_linear_algebra">Peterson, Pedersen, 2006</a>)<br />
<a href="#02_linear_algebra">Shilov, 1977</a></p>
<h2 id="0201_scalars_vectors_matrices_and_tensors">02.01. Scalars, Vectors, Matrices and Tensors<a class="headerlink" href="#0201_scalars_vectors_matrices_and_tensors" title="Permanent link">&para;</a></h2>
<ul>
<li>scalars</li>
<li>vectors: <script type="math/tex">x</script>
</li>
</ul>
<p>
<script type="math/tex; mode=display">
    x = 
    \begin{bmatrix}
        x_1 \\ x_2 \\ \vdots \\ x_n
    \end{bmatrix}
    \tag{2.1}
</script>
</p>
<ul>
<li>matrices: <script type="math/tex">A</script>
</li>
</ul>
<p>
<script type="math/tex; mode=display">
    A = 
    \begin{bmatrix}
        A_{1,1} & A_{1,2} \\
        A_{2,1} & A_{2,2} \\
        A_{3,1} & A_{3,2}
    \end{bmatrix} \Rightarrow A^T =
    \begin{bmatrix}
        A_{1,1} & A_{2,1} & A_{3,1} \\
        A_{1,2} & A_{2,2} & A_{3,2}
    \end{bmatrix}
</script>
</p>
<ul>
<li>tensors: <script type="math/tex">\mathsf{A}</script>
</li>
</ul>
<p>
<script type="math/tex">A_{i, j, k}</script>
</p>
<p>def transposition
<script type="math/tex; mode=display"> (A^T)_{i,j} = A_{j,i} \tag{2.3} </script>
</p>
<p>
<script type="math/tex">C = A + B</script> where <script type="math/tex">C_{i,j} = A_{i,j} + B_{i,j}</script>
</p>
<p>
<script type="math/tex">D = a \cdot B + c</script> where <script type="math/tex">D_{i,j} = a \cdot B_{i,j} + c</script>
</p>
<p>
<script type="math/tex">C = A + b</script> where <script type="math/tex">C_{i,j} = A_{i,j} + b_j</script>
<br />
<strong>broadacsting</strong></p>
<h2 id="0202_multiplying_matrices_and_vectors">02.02. Multiplying Matrices and Vectors<a class="headerlink" href="#0202_multiplying_matrices_and_vectors" title="Permanent link">&para;</a></h2>
<p><strong>matrix product</strong><br />
<script type="math/tex; mode=display"> C = AB \tag{2.4} </script>
<script type="math/tex; mode=display"> C_{i,j} = \sum_k{A_{i,k} B_{k,j}} \tag{2.5} </script>
</p>
<p><strong>element-wise product</strong> (aka, <strong>Hadamard product</strong>)<br />
<script type="math/tex; mode=display"> C = A \odot B </script>
<script type="math/tex; mode=display"> C_{i,j} = A_{i,j} B_{i,j} </script>
</p>
<p><strong>dot product</strong>: 
<script type="math/tex">x \cdot y = x^T y</script>
</p>
<p>
<script type="math/tex; mode=display"> x^T y = y^T x \tag{2.8} </script>
</p>
<p>
<script type="math/tex; mode=display"> (AB)^T = B^T A^T \tag{2.9} </script>
</p>
<p>
<script type="math/tex; mode=display"> x^T y = (x^T y)^T = y^T x \tag{2.10} </script>
</p>
<p>a system of linear equations:</p>
<p>
<script type="math/tex; mode=display"> Ax = b \tag{2.11} </script>
</p>
<p>where <script type="math/tex">A \in \mathbb{R}^{m \times n}</script>, <script type="math/tex">b \in \mathbb{R}^m</script>, <script type="math/tex">x \in \mathbb{R}^n</script>
</p>
<p>
<script type="math/tex; mode=display">
    \begin{align*}
        A_{1,:} x &= b_1 \tag{2.12} \\
        A_{2,:} x &= b_2 \tag{2.13} \\
        \vdots \tag{2.14} \\
        A_{m,:} x &= b_m \tag{2.15} \\
    \end{align*}
</script>
</p>
<p>
<script type="math/tex; mode=display"> 
    \begin{align*}
        A_{1,1} x_1 + A_{1,2} x_2 + \cdots + A_{1,n} x_n &= b_1 \tag{2.16} \\
        A_{2,1} x_1 + A_{2,2} x_2 + \cdots + A_{2,n} x_n &= b_2 \tag{2.17} \\
        \vdots \tag{2.18} \\
        A_{m,1} x_1 + A_{m,2} x_2 + \cdots + A_{m,n} x_n &= b_m \tag{2.19} 
    \end{align*}
</script>
</p>
<h2 id="0203_identity_and_inverse_matrices">02.03. Identity and Inverse Matrices<a class="headerlink" href="#0203_identity_and_inverse_matrices" title="Permanent link">&para;</a></h2>
<p><strong>matrix inversion</strong></p>
<p><strong>identity matrix</strong> <script type="math/tex">I_n \in \mathbb{R}^{n \times n}</script>
</p>
<p>
<script type="math/tex; mode=display"> \forall x \in \mathbb{R}^n, \quad I_n x = x \tag{2.20} </script>
</p>
<p>
<script type="math/tex; mode=display"> A^{-1} A = I_n \tag{2.21} </script>
</p>
<h2 id="0204_linear_dependence_and_span">02.04. Linear Dependence and Span<a class="headerlink" href="#0204_linear_dependence_and_span" title="Permanent link">&para;</a></h2>
<p>in order for <script type="math/tex">A^{-1}</script> to exist</p>
<p>
<script type="math/tex; mode=display"> Ax = b \tag{2.11} </script>
</p>
<p>eq.2.11 must have one solution</p>
<p>possible to have no solution / infinitely many solutions<br />
<strong>NOT</strong> possible to have more than 1 but less than infinitely solutions</p>
<p>if both <script type="math/tex">x</script> and <script type="math/tex">y</script> are solutions then</p>
<p>
<script type="math/tex; mode=display"> z = \alpha x + (1 - \alpha) y \tag{2.26} </script>
</p>
<p>is also solution for any real <script type="math/tex">\alpha</script>
</p>
<p>
<script type="math/tex; mode=display"> Ax = \sum_i{x_i A_{:,i}} \tag{2.27} </script>
</p>
<p><strong>linear combination</strong></p>
<p>
<script type="math/tex">\{ v^{(1)}, \cdots, v^{(n)} \}</script>
</p>
<p>
<script type="math/tex; mode=display"> \sum_i{c_i v^{(i)}} \tag{2.28} </script>
</p>
<p><strong>span</strong> </p>
<p>
<script type="math/tex">Ax = b</script> have solution for all vals of <script type="math/tex">b \in \mathbb{R}^m</script>
</p>
<h2 id="0205_norms">02.05. Norms<a class="headerlink" href="#0205_norms" title="Permanent link">&para;</a></h2>
<p><strong>norm</strong>: fn that measure size of vectors </p>
<p>
<script type="math/tex">L^p</script> norm is given by</p>
<p>
<script type="math/tex; mode=display"> ||x||_p = \Bigg( \sum_i{|x_i|^p} \Bigg)^{\frac{1}{p}} \tag{2.30} </script>
</p>
<p>for <script type="math/tex">p \in \mathbb{R}</script>, <script type="math/tex">p \ge 1</script>
</p>
<ul>
<li>
<script type="math/tex">f(x) = 0</script> &rArr; <script type="math/tex">x=0</script>
</li>
<li>
<script type="math/tex">f(x + y) \le f(x) + f(y)</script> (<strong>triangle inequality</strong>)</li>
<li>
<script type="math/tex">\forall\alpha \in \mathbb{R}</script>, <script type="math/tex">f(\alpha x) = |\alpha| f(x)</script>
</li>
</ul>
<p>
<script type="math/tex">L^2</script> norm, with <script type="math/tex">p=2</script> ka <strong>Euclidean norm</strong> (<script type="math/tex">||x||</script>)</p>
<p>
<script type="math/tex">L^1</script> norm</p>
<p>
<script type="math/tex; mode=display"> ||x||_1 = \sum_i{|x_i|} \tag{2.31} </script>
</p>
<p>
<script type="math/tex">L^\infty</script> norm (aka, <strong>max norm</strong>)</p>
<p>
<script type="math/tex; mode=display"> ||x||_\infty = \max_i{|x_i|} \tag{2.32} </script>
</p>
<p><strong>Frobenius norm</strong></p>
<p>
<script type="math/tex; mode=display"> ||A||_F = \sqrt{\sum_{i,j}{A_{i,j}^2}} \tag{2.33} </script>
is analogous to <script type="math/tex">L^2</script> norm of vec</p>
<p>
<script type="math/tex; mode=display"> x^T y = ||x||_2 ||y||_2 \cos{\theta} \tag{2.34} </script>
</p>
<p>where <script type="math/tex">\theta</script> is angle between <script type="math/tex">x</script>, <script type="math/tex">y</script>
</p>
<h2 id="0206_special_kinds_of_matricesand_vectors">02.06. Special Kinds of Matricesand Vectors<a class="headerlink" href="#0206_special_kinds_of_matricesand_vectors" title="Permanent link">&para;</a></h2>
<p><strong>diagonal</strong> <script type="math/tex">D</script>
<br />
if and only if <script type="math/tex">D_{i,j}=0</script> for all <script type="math/tex">i \ne j</script>
</p>
<pre><code class="py">np.random.seed(123)
v = np.random.randint(9, size=5)
np.diag(v)
</code></pre>

<pre><code class="py">array([[2, 0, 0, 0, 0],
       [0, 2, 0, 0, 0],
       [0, 0, 6, 0, 0],
       [0, 0, 0, 1, 0],
       [0, 0, 0, 0, 3]])
</code></pre>

<p>
<script type="math/tex; mode=display"> \text{diag}(v) x = v \odot x </script>
</p>
<pre><code class="py">np.matmul(np.diag(v), x)

np.multiply(v, x)
</code></pre>

<p>
<script type="math/tex; mode=display"> \text{diag}(v)^{-1} = \text{diag}\Bigg( \bigg[\frac{1}{v_1}, \ldots, \frac{1}{v_n} \bigg]^T \Bigg) </script>
</p>
<pre><code class="py">np.linalg.inv(diag_v)
np.diag([1/vi for vi in v])
</code></pre>

<p><strong>symmetric</strong> mat</p>
<p>
<script type="math/tex; mode=display"> A = A^T \tag{2.35} </script>
</p>
<p><strong>unit vector</strong>: vec w/ <strong>unit norm</strong>:</p>
<p>
<script type="math/tex; mode=display"> ||x||_2 = 1 \tag{2.36} </script>
</p>
<p>vec <script type="math/tex">x</script> and vec <script type="math/tex">y</script>
<strong>orthogonal</strong> to each other if <script type="math/tex">x^T y = 0</script>
</p>
<p>orthogonal &amp; unit norm <strong>orthonormal</strong></p>
<p><strong>orthogonal matrix</strong></p>
<p>
<script type="math/tex; mode=display"> A^T A = A A^T = I \tag{2.37} </script>
</p>
<p>implies 
<script type="math/tex; mode=display"> A^{-1} = A^T \tag{2.38} </script>
</p>
<pre><code class="py">from scipy.stats import ortho_group

A = ortho_group.rvs(dim = 3)

np.set_printoptions(suppress=True)
AT_A = np.matmul(A, A.T)
</code></pre>

<h2 id="0207_eigendecomposition">02.07. Eigendecomposition<a class="headerlink" href="#0207_eigendecomposition" title="Permanent link">&para;</a></h2>
<p><strong>eigendecomposition</strong>: into set of eigenvectors &amp; eigenvalues</p>
<p><strong>eigenvector</strong> of square mat <script type="math/tex">A</script> is non-zero vec <script type="math/tex">v</script>
<script type="math/tex; mode=display"> Av = \lambda v \tag{2.39} </script>
</p>
<p><strong>eigenvalue</strong> <script type="math/tex">\lambda</script>
</p>
<pre><code class="py">l, V = np.linalg.eig(A)
</code></pre>

<p><a href="https://raw.githubusercontent.com/shumez/DeepLearningBook/master/02/fig/0203.png"><img alt="Fig.2.3" src="https://raw.githubusercontent.com/shumez/DeepLearningBook/master/02/fig/0203.png" /></a></p>
<p>mat <script type="math/tex">A</script> has <script type="math/tex">n</script> linearly independent eigenvec <script type="math/tex">\{ v^{(1)}, \cdots, v^{(n)} \}</script>,
w/ corresponding eigenval <script type="math/tex">\{ \lambda_1, \cdots, \lambda_n \}</script>
</p>
<p>mat <script type="math/tex">V</script>: <script type="math/tex">V = [ v^{(1)}, \cdots, v^{(b)} ] </script>,<br />
vec <script type="math/tex">\lambda</script>: <script type="math/tex">[\lambda_1, \cdots, \lambda_n]^T</script>
</p>
<p><strong>eigendecomposition</strong></p>
<p>
<script type="math/tex; mode=display"> A = V\text{diag}(\lambda) V^{-1} \tag{2.40} </script>
</p>
<pre><code class="py">np.linalg.multi_dot([V, np.diag(l), np.linalg.inv(V)])
</code></pre>

<p>
<script type="math/tex; mode=display">
    \begin{align*}
        A &= Q \Lambda Q^T \\
        &= 
        \begin{bmatrix}
            v_1 & v_2 & \cdots & v_n
        \end{bmatrix}
        \begin{bmatrix}
            \lambda_1 & 0 & \cdots & 0 \\
            0 & \lambda_2 & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & \lambda_n 
        \end{bmatrix}
        \begin{bmatrix}
            v_1 \\ v_2 \\ \vdots \\ v_n
        \end{bmatrix}
    \end{align*}
    \tag{2.41} 
</script>
</p>
<p>eigenvectors, orthogonal mat <script type="math/tex">Q</script> of <script type="math/tex">A</script>,<br />
eigenvalues, diagonal mat <script type="math/tex">\Lambda</script> of <script type="math/tex">A</script>
</p>
<p>eigenvalue <script type="math/tex">\Lambda_{i,i}</script> is associated w eigenvector <script type="math/tex">Q_{:,i}</script>
</p>
<p>
<script type="math/tex">f(x) = x^T Ax \quad (||x|| = 1)</script>
</p>
<ul>
<li><strong>positive definite</strong>: mat whose eigenval are all positive</li>
<li><strong>positive semidefinite</strong>: eigenvals are all positive / zero</li>
<li><strong>negative definite</strong>: eigenvals all negative</li>
<li><strong>negative semidefinite</strong>: eigenvals negative / zero</li>
</ul>
<p>positive semideginite mat <br />
<script type="math/tex">\forall x, x^T Ax \ge 0</script>
</p>
<p>positive definite mat<br />
<script type="math/tex">x^T Ax = 0 \Rightarrow x = 0</script>
</p>
<h2 id="0208_singular_value_decomposition">02.08. Singular Value Decomposition<a class="headerlink" href="#0208_singular_value_decomposition" title="Permanent link">&para;</a></h2>
<p><strong>singular value decomposition</strong> (SVD)<br />
<strong>singular vecs</strong>, <strong>singular vals</strong></p>
<pre><code class="py">u, s, vh = np.linalg.svd(A, full_matrices=False)
</code></pre>

<p>eigendecomposition
<script type="math/tex; mode=display"> A = V \text{diag}(\lambda) V^{-1} \tag{2.42} </script>
</p>
<p>singular value decomposition
<script type="math/tex; mode=display"> A = UDV^T \tag{2.43} </script>
</p>
<p>
<script type="math/tex">A</script>
<script type="math/tex">m \times n</script> mat<br />
<strong>left-singular vec</strong> <script type="math/tex">U</script>
<script type="math/tex">m \times m</script> orthogonal mat<br />
<strong>singular values</strong> <script type="math/tex">D</script>
<script type="math/tex">m \times n</script> diagonal mat<br />
<strong>right-singular vec</strong> <script type="math/tex">V</script>
<script type="math/tex">n \times n</script> orthogonal mat</p>
<h2 id="0209_the_moore-penrose_pseudoinverse">02.09. The Moore-Penrose Pseudoinverse<a class="headerlink" href="#0209_the_moore-penrose_pseudoinverse" title="Permanent link">&para;</a></h2>
<p>
<script type="math/tex; mode=display">
    \begin{align*}
        Ax &= y \tag{2.44} \\
        x &= By \tag{2.45}
    \end{align*}
</script>
</p>
<p>left-inverse <script type="math/tex">B</script> of mat <script type="math/tex">A</script>
</p>
<p><strong>Moore-Penrose pseudoinverse</strong></p>
<p>pseudoinverse of <script type="math/tex">A</script> is defined as </p>
<p>
<script type="math/tex; mode=display">  A^+ = \lim\limits_{\alpha \to 0} (A^T A + \alpha I)^{-1} A^T \tag{2.46} </script>
</p>
<p>
<script type="math/tex; mode=display"> A^+ = V D^+ U^+ \tag{2.47} </script>
</p>
<p>~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub></p>
<h2 id="0210_the_trace_operator">02.10. The Trace Operator<a class="headerlink" href="#0210_the_trace_operator" title="Permanent link">&para;</a></h2>
<p>
<script type="math/tex; mode=display"> \mathrm{Tr}(A) = \sum_i{A_{i,i}} \tag{2.48} </script>
</p>
<p>Frobenius norm </p>
<p>
<script type="math/tex; mode=display"> ||A||_F = \sqrt{\mathrm{Tr}(A A^T)} \tag{2.49} </script>
</p>
<p>
<script type="math/tex; mode=display"> \mathrm{Tr}(A) = \mathrm{Tr}(A^T) \tag{2.50} </script>
</p>
<p>
<script type="math/tex; mode=display"> \mathrm{Tr}(ABC) = \mathrm{Tr}(CAB) = \mathrm{Tr}(BCA) \tag{2.51} </script>
</p>
<p>more generally</p>
<p>
<script type="math/tex; mode=display"> \mathrm{Tr}\bigg(\prod_{i=1}^n{F^{(i)}}\bigg) = \mathrm{Tr}\bigg( F^{(n)} \prod_{i=1}^{n-1}{F^{(i)}} \bigg) \tag{2.52} </script>
</p>
<p>
<script type="math/tex">A \in \mathbb{R}^{m \times n}</script>, <script type="math/tex">B \in \mathbb{R}^{n \times m}</script>
<br />
<script type="math/tex; mode=display"> \mathrm{Tr}(AB) = \mathrm{Tr}(BA) \tag{2.53} </script>
</p>
<p>though <script type="math/tex">AB \in \mathbb{R}^{m \times m}</script>, <script type="math/tex">BA \in \mathbb{R}^{n \times n}</script>
</p>
<h2 id="0211_the_determinant">02.11. The Determinant<a class="headerlink" href="#0211_the_determinant" title="Permanent link">&para;</a></h2>
<p>
<script type="math/tex">\det{(A)}</script>
</p>
<h2 id="0212_example_principal_components_analysis">02.12. Example: Principal Components Analysis<a class="headerlink" href="#0212_example_principal_components_analysis" title="Permanent link">&para;</a></h2>
<p><strong>principal components analysis</strong> (PCA)</p>
<p>
<script type="math/tex">m</script> points <script type="math/tex">\{ x^{(1)}, \ldots, x^{(m)} \}</script> in <script type="math/tex">\mathbb{R}^n</script>
</p>
<p>lower-dim version<br />
corresponding code vec <script type="math/tex">c^{(i)} \in \mathbb{R}^l</script>
<br />
<script type="math/tex">l < n</script>
</p>
<p>encoding fn <script type="math/tex">f(x) = c</script>
<br />
decoding fn <script type="math/tex">x \approx g(f(x))</script>
</p>
<p>map code back into <script type="math/tex">\mathbb{R}^n</script>
<br />
let <script type="math/tex">g(c) = Dc</script>, where <script type="math/tex">D \in \mathbb{R}^{n \times l}</script>
</p>
<p>
<script type="math/tex">L^2</script> norm:<br />
<script type="math/tex; mode=display"> c^* \arg_c\min{||x - g(c)||_2} \tag{2.54} </script>
</p>
<p>
<script type="math/tex; mode=display"> c^* = \arg_c\min{||x - g(c)||_2^2} \tag{2.55} </script>
</p>
<p>
<script type="math/tex; mode=display">
    \begin{align*} 
        &(x - g(c))^T (x - g(c)) \tag{2.56} \\
        &= x^T x - x^T g(c) - g(c)^T x + g(c)^T g(c) \tag{2.57} \\
        &= x^T x - 2x^T g(c) + g(c)^T g(c) \tag{2.58}
    \end{align*} 
</script>
</p>
<p>~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub> ~ <sub> ~ </sub></p>
<h2 id="_1"><a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<!-- toc -->

<!-- ref -->

<!-- fig -->

<!-- term -->

<style type="text/css">
    img{width: 51%; float: right;}
</style>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../03/" class="btn btn-neutral float-right" title="03. Probability and Information Theory">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../01/" class="btn btn-neutral" title="01. Introduction"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
      <p>Copyright &copy; 2018 - 2019 <a href="https://github.com/shumez">shumez</a>
</p>
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../01/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../03/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>

<!--
MkDocs version : 1.0.2
Build Date UTC : 2019-06-06 08:14:52
-->

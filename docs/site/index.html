<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="None">
  
  <link rel="shortcut icon" href="img/favicon.ico">
  <title>Home - Deep Learning Book</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="css/theme.css" type="text/css" />
  <link rel="stylesheet" href="css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Home";
    var mkdocs_page_input_path = "index.md";
    var mkdocs_page_url = "/";
  </script>
  
  <script src="js/jquery-2.1.1.min.js" defer></script>
  <script src="js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="." class="icon icon-home"> Deep Learning Book</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1 current">
		
    <a class="current" href=".">Home</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#contents">Contents</a></li>
    

    <li class="toctree-l2"><a href="#resources">Resources</a></li>
    

    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="01/">01. Introduction</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="02/">02. Linear Algebra</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href=".">Deep Learning Book</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".">Docs</a> &raquo;</li>
    
      
    
    <li>Home</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <!--
Filename:   note.md
Project:    /Users/shume/Developer/DeepLearningBook
Author:     shumez <https://github.com/shumez>
Created:    2018-07-06 13:36:3
Modified:   2019-06-01 20:38:45
-----
Copyright (c) 2019 shumez
-->

<h1 id="deep_learning_book"><a href="https://www.deeplearningbook.org">Deep Learning Book</a><a class="headerlink" href="#deep_learning_book" title="Permanent link">&para;</a></h1>
<p><a href="https://images-na.ssl-images-amazon.com/images/I/61fim5QqaqL._SX373_BO1,204,203,200_.jpg"><img alt="Cover" src="https://images-na.ssl-images-amazon.com/images/I/61fim5QqaqL._SX373_BO1,204,203,200_.jpg" /></a></p>
<h2 id="contents">Contents<a class="headerlink" href="#contents" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="#contents">Part 0.</a><ul>
<li><a href="01/">01. Introduction</a>
    <!-- * <a href="">1.1 Who Should Read This Book?</a> -->
    <!-- * <a href="">1.2 Historical Trends in Deep Learning</a> --></li>
</ul>
</li>
<li><a href="#contents">Part I. Applied Math and Machine Learning Basics</a><ul>
<li><a href="02/">02. Linear Algebra</a>
    <!-- - 2.1 Scalars,Vectors, MatricesandTensors -->
    <!-- - 2.2 Multiplying Matrices and Vectors -->
    <!-- - 2.3 Identity and Inverse Matrices -->
    <!-- - 2.4 Linear Dependence and Span -->
    <!-- - 2.5 Norms -->
    <!-- - 2.6 Special Kinds of Matricesand Vectors -->
    <!-- - 2.7 Eigendecomposition -->
    <!-- - 2.8 Singular Value Decomposition -->
    <!-- - 2.9 The Moore-Penrose Pseudoinverse -->
    <!-- - 2.10 The Trace Operator -->
    <!-- - 2.11 The Determinant -->
    <!-- - 2.12 Example: PrincipalComponentsAnalysis --></li>
<li><a href="03/">03. Probability and Information Theory</a>
    <!-- - 3.1 Why Probability? -->
    <!-- - 3.2 Random Variables -->
    <!-- - 3.3 Probability Distributions -->
    <!-- - 3.4 Marginal Probability -->
    <!-- - 3.5 Conditional Probability -->
    <!-- - 3.6 TheChainRuleofConditionalProbabilities -->
    <!-- - 3.7 Independence and Conditional Independence -->
    <!-- - 3.8 Expectation,VarianceandCovariance -->
    <!-- - 3.9 Common Probability Distributions -->
    <!-- - 3.10 UsefulPropertiesofCommonFunctions -->
    <!-- - 3.11 Bayesâ€™ Rule -->
    <!-- - 3.12 Technical Details of Continuous Variables -->
    <!-- - 3.13 Information Theory -->
    <!-- - 3.14 Structured Probabilistic Models --></li>
<li><a href="04/">04. Numerical Computation</a>
    <!-- - 4.1 Overflow and Underflow -->
    <!-- - 4.2 Poor Conditioning -->
    <!-- - 4.3 Gradient-Based Optimization -->
    <!-- - 4.4 Constrained Optimization . . -->
    <!-- - 4.5 Example: Linear Least Squares --></li>
<li><a href="05/">05. Machine Learning Basics</a>
    <!-- - 5.1 Learning Algorithms -->
    <!-- - 5.2 Capacity,OverfittingandUnderfitting -->
    <!-- - 5.3 HyperparametersandValidationSets -->
    <!-- - 5.4 Estimators, Bias and Variance -->
    <!-- - 5.5 Maximum Likelihood Estimation -->
    <!-- - 5.6 BayesianStatistics -->
    <!-- - 5.7 Supervised Learning Algorithms -->
    <!-- - 5.8 Unsupervised Learning Algorithms -->
    <!-- - 5.9 Stochastic Gradient Descent -->
    <!-- - 5.10 Building a Machine Learning Algorithm -->
    <!-- - 5.11 ChallengesMotivatingDeepLearning --></li>
</ul>
</li>
<li><a href="#contents">Part II. Deep Networks: Modern Practices</a><ul>
<li><a href="06/">06. Deep Feedforward Networks</a>
    <!-- - 6.1 Example: Learning XOR -->
    <!-- - 6.2 Gradient-Based Learning -->
    <!-- - 6.3 Hidden Units -->
    <!-- - 6.4 Architecture Design -->
    <!-- - 6.5 Back-Propagation and Other Differentiation Algorithms -->
    <!-- - 6.6 Historical Notes --></li>
<li><a href="07/">07. Regularization for Deep Learning</a>
    <!-- - 7.1 Parameter Norm Penalties -->
    <!-- - 7.2 Norm Penalties as Constrained Optimization -->
    <!-- - 7.3 Regularization and Under-Constrained Problems -->
    <!-- - 7.4 Dataset Augmentation -->
    <!-- - 7.5 Noise Robustness -->
    <!-- - 7.6 Semi-Supervised Learning -->
    <!-- - 7.7 Multi-Task Learning -->
    <!-- - 7.8 EarlyStopping -->
    <!-- - 7.9 Parameter Tying and Parameter Sharing -->
    <!-- - 7.10 Sparse Representations -->
    <!-- - 7.11 BaggingandOtherEnsembleMethods -->
    <!-- - 7.12 Dropout -->
    <!-- - 7.13 Adversarial Training -->
    <!-- - 7.14 Tangent Distance, Tangent Prop, and Manifold Tangent Classifier --></li>
<li><a href="08/">08. Optimization for Training Deep Models</a>
    <!-- - 8.1 HowLearningDiffersfromPureOptimization -->
    <!-- - 8.2 Challenges in Neural Network Optimization -->
    <!-- - 8.3 Basic Algorithms -->
    <!-- - 8.4 Parameter Initialization Strategies -->
    <!-- - 8.5 AlgorithmswithAdaptiveLearningRates -->
    <!-- - 8.6 Approximate Second-Order Methods -->
    <!-- - 8.7 Optimization Strategies and Meta-Algorithms --></li>
<li><a href="09/">09. Convolutional Networks</a>
    <!-- - 9.1 The Convolution Operation -->
    <!-- - 9.2 Motivation -->
    <!-- - 9.3 Pooling -->
    <!-- - 9.4 Convolution and Pooling as an Infinitely Strong Prior -->
    <!-- - 9.5 Variants of the Basic Convolution Function -->
    <!-- - 9.6 Structured Outputs -->
    <!-- - 9.7 Data Types -->
    <!-- - 9.8 Efficient Convolution Algorithms -->
    <!-- - 9.9 Random or Unsupervised Features -->
    <!-- - 9.10 The Neuroscientific Basis for Convolutional Networks -->
    <!-- - 9.11 Convolutional Networks and the History of Deep Learning --></li>
<li><a href="10/">10. Sequence Modeling: Recurrent and Recursive Nets</a>
    <!-- - 10.1 Unfolding Computational Graphs -->
    <!-- - 10.2 Recurrent Neural Networks -->
    <!-- - 10.3 Bidirectional RNNs -->
    <!-- - 10.4 Encoder-Decoder Sequence-to-Sequence Architectures -->
    <!-- - 10.5 Deep Recurrent Networks -->
    <!-- - 10.6 Recursive Neural Networks -->
    <!-- - 10.7 TheChallengeofLong-TermDependencies -->
    <!-- - 10.8 Echo State Networks -->
    <!-- - 10.9 Leaky Units and Other Strategies for Multiple Time Scales -->
    <!-- - 10.10 The Long Short-Term Memory and Other Gated RNNs -->
    <!-- - 10.11 Optimization for Long-Term Dependencies -->
    <!-- - 10.12 Explicit Memory --></li>
<li><a href="11/">11. Practical Methodology</a>
    <!-- - 11.1 Performance Metrics -->
    <!-- - 11.2 Default Baseline Models -->
    <!-- - 11.3 Determining Whether to Gather More Data -->
    <!-- - 11.4 Selecting Hyperparameters -->
    <!-- - 11.5 Debugging Strategies -->
    <!-- - 11.6 Example: Multi-Digit Number Recognition --></li>
<li><a href="12/">12. Applications</a>
    <!-- - 12.1 Large-Scale Deep Learning -->
    <!-- - 12.2 Computer Vision -->
    <!-- - 12.3 Speech Recognition -->
    <!-- - 12.4 Natural Language Processing -->
    <!-- - 12.5 Other Applications --></li>
</ul>
</li>
<li><a href="#contents">Part III. Deep Learning Research</a><ul>
<li><a href="13/">13. Linear Factor Models</a>
    <!-- - 13.1 ProbabilisticPCAandFactorAnalysis -->
    <!-- - 13.2 IndependentComponentAnalysis(ICA) -->
    <!-- - 13.3 Slow Feature Analysis -->
    <!-- - 13.4 Sparse Coding -->
    <!-- - 13.5 Manifold Interpretation of PCA --></li>
<li><a href="14/">14. Autoencoders</a>
    <!-- - 14.1 Undercomplete Autoencoders -->
    <!-- - 14.2 Regularized Autoencoders -->
    <!-- - 14.3 Representational Power, Layer Size and Depth -->
    <!-- - 14.4 Stochastic Encoders and Decoders -->
    <!-- - 14.5 Denoising Autoencoders -->
    <!-- - 14.6 LearningManifoldswithAutoencoders -->
    <!-- - 14.7 Contractive Autoencoders -->
    <!-- - 14.8 Predictive Sparse Decomposition -->
    <!-- - 14.9 Applications of Autoencoders --></li>
<li><a href="15/">15. Representation Learning</a>
    <!-- - 15.1 Greedy Layer-Wise Unsupervised Pretraining -->
    <!-- - 15.2 TransferLearningandDomainAdaptation -->
    <!-- - 15.3 Semi-Supervised Disentangling of Causal Factors -->
    <!-- - 15.4 Distributed Representation -->
    <!-- - 15.5 Exponential Gains from Depth -->
    <!-- - 15.6 Providing Clues to Discover Underlying Causes --></li>
<li><a href="16/">16. Structured Probabilistic Models for Deep Learning</a>
    <!-- - 16.1 TheChallengeofUnstructuredModeling -->
    <!-- - 16.2 UsingGraphstoDescribeModelStructure -->
    <!-- - 16.3 Sampling from Graphical Models -->
    <!-- - 16.4 Advantages of Structured Modeling -->
    <!-- - 16.5 Learning about Dependencies -->
    <!-- - 16.6 InferenceandApproximateInference -->
    <!-- - 16.7 The Deep Learning Approach to Structured Probabilistic Models --></li>
<li><a href="17/">17. Monte Carlo Methods</a>
    <!-- - 17.1 Sampling and Monte Carlo Methods -->
    <!-- - 17.2 Importance Sampling -->
    <!-- - 17.3 Markov Chain Monte Carlo Methods -->
    <!-- - 17.4 Gibbs Sampling -->
    <!-- - 17.5 The Challenge of Mixing between Separated Modes --></li>
<li><a href="18/">18. Confronting the Partition Function</a>
    <!-- - 18.1 The Log-Likelihood Gradient -->
    <!-- - 18.2 Stochastic Maximum Likelihood and Contrastive Divergence -->
    <!-- - 18.3 Pseudolikelihood -->
    <!-- - 18.4 Score Matching and Ratio Matching -->
    <!-- - 18.5 Denoising Score Matching -->
    <!-- - 18.6 Noise-Contrastive Estimation -->
    <!-- - 18.7 Estimating the Partition Function --></li>
<li><a href="19/">19. Approximate Inference</a>
    <!-- - 19.1 Inference as Optimization -->
    <!-- - 19.2 Expectation Maximization -->
    <!-- - 19.3 MAP Inference and Sparse Coding -->
    <!-- - 19.4 Variational Inference and Learning -->
    <!-- - 19.5 Learned Approximate Inference --></li>
<li><a href="20/">20. Deep Generative Models</a>
    <!-- - 20.1 Boltzmann Machines -->
    <!-- - 20.2 Restricted Boltzmann Machines -->
    <!-- - 20.3 Deep Belief Networks -->
    <!-- - 20.4 Deep Boltzmann Machines -->
    <!-- - 20.5 BoltzmannMachinesforReal-Valued Data -->
    <!-- - 20.6 Convolutional Boltzmann Machines -->
    <!-- - 20.7 Boltzmann Machines for Structured or Sequential Outputs -->
    <!-- - 20.8 Other Boltzmann Machines -->
    <!-- - 20.9 Back-Propagation through Random Operations -->
    <!-- - 20.10 Directed Generative Nets -->
    <!-- - 20.11 Drawing Samples from Autoencoders -->
    <!-- - 20.12 Generative Stochastic Networks -->
    <!-- - 20.13 Other Generation Schemes -->
    <!-- - 20.14 Evaluating Generative Models -->
    <!-- - 20.15 Conclusion --></li>
</ul>
</li>
</ul>
<h2 id="resources">Resources<a class="headerlink" href="#resources" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://www.deeplearningbook.org">Deep Learning Book</a></li>
<li><a href="https://www.deeplearningbook.org/lecture_slides.html">Lecture</a></li>
<li><a href="https://github.com/janishar/mit-deep-learning-book-pdf">github</a></li>
</ul>
<h2 id="_1"><a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<!-- toc -->

<!-- ref -->

<!-- fig -->

<style type="text/css">
    img{width: 51%; float: right;}
</style>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="01/" class="btn btn-neutral float-right" title="01. Introduction">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
      <p>Copyright &copy; 2018 - 2019 <a href="https://github.com/shumez">shumez</a>
</p>
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
      
        <span style="margin-left: 15px"><a href="01/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '.';</script>
    <script src="js/theme.js" defer></script>
      <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
      <script src="search/main.js" defer></script>

</body>
</html>

<!--
MkDocs version : 1.0.2
Build Date UTC : 2019-06-02 09:32:38
-->

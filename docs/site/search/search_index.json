{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Deep Learning Book \u00b6 Contents \u00b6 Part 0. 01. Introduction Part I. Applied Math and Machine Learning Basics 02. Linear Algebra 03. Probability and Information Theory 04. Numerical Computation 05. Machine Learning Basics Part II. Deep Networks: Modern Practices 06. Deep Feedforward Networks 07. Regularization for Deep Learning 08. Optimization for Training Deep Models 09. Convolutional Networks 10. Sequence Modeling: Recurrent and Recursive Nets 11. Practical Methodology 12. Applications Part III. Deep Learning Research 13. Linear Factor Models 14. Autoencoders 15. Representation Learning 16. Structured Probabilistic Models for Deep Learning 17. Monte Carlo Methods 18. Confronting the Partition Function 19. Approximate Inference 20. Deep Generative Models Resources \u00b6 Deep Learning Book Lecture github \u00b6 img{width: 51%; float: right;}","title":"Home"},{"location":"#contents","text":"Part 0. 01. Introduction Part I. Applied Math and Machine Learning Basics 02. Linear Algebra 03. Probability and Information Theory 04. Numerical Computation 05. Machine Learning Basics Part II. Deep Networks: Modern Practices 06. Deep Feedforward Networks 07. Regularization for Deep Learning 08. Optimization for Training Deep Models 09. Convolutional Networks 10. Sequence Modeling: Recurrent and Recursive Nets 11. Practical Methodology 12. Applications Part III. Deep Learning Research 13. Linear Factor Models 14. Autoencoders 15. Representation Learning 16. Structured Probabilistic Models for Deep Learning 17. Monte Carlo Methods 18. Confronting the Partition Function 19. Approximate Inference 20. Deep Generative Models","title":"Contents"},{"location":"#resources","text":"Deep Learning Book Lecture github","title":"Resources"},{"location":"01/","text":"01. Introduction \u00b6 Contents \u00b6 01.00. 01.01. Who Should Read This Book? 01.02. Historical Trends in Deep Learning 01.02.01. The Many Names and Changing Fortunes of Neural Networks 01.02.02. Increasing Dataset Sizes 01.02.03. INcreasing Model Sizes 01.02.04. Increasing Accuracy, Complexity and Real-World Impact 01.00 \u00b6 ancient Greece Galatea by Pygmalion Talos by Daedalus Pandora by Hephaestus AI deep learning logistic regression naive Bayes 01.01. Who Should Read This Book? \u00b6 01.02. Historical Trends in Deep Learning \u00b6 01.02.01. The Many Names and Changing Fortunes of Neural Networks \u00b6 cybernetics in 1940s-1960s connectionism in 1980s-1990s deep learning in 2006 artificial neural networks (ANNs) f(x, w) = x_1 x_1 + \\cdots + x_n w_n McCulloch-Pitts Neuron ( McCulloch, Pitts, 1943 ) perceptron ( Rosenblatt, 1958 , 1962 ) ADALINE ( Widrow, Hoff, 1960 ) stochasic gradient descent linear models limitation: connot learn the XOR fn \\begin{cases} f([0,1], w) = 1 \\\\ f([1,0], w) = 1 \\\\ f([1,1], w) = 1 \\\\ f([0,0], w) = 0 \\end{cases} Minsky, Papert, 1969 Olshausen, Field, 2005 ferrets brain ( Von Melshner, 2000 ) Neocognitron ( Fukushima, 1980 ) \u2192 modern convolutional network ( LeCun, 1998 ) rectified linear unit original Cognitron ( Fukushima, 1975 ) Nair, Hinton, 2010 , Glorot, 2011 citing neuroscience, Jarrett, 2009 citing more engineering-oriented 1980s: second wave of NN movement connectionism / parallel distributed processing ( Rumelhart, 1986 , McClelland, 1995 ) models of cognition grounded in neural implementation ( Touretzky, Minton, 1985 ), reviving work og Fonald Hebb in 1940s ( Hebb, 1949 ) connectionsim: large num of sumple computational units can ahchive intelligent behavior distributed representation ( Hinton, 1986 ) e.g., 3 neurons describing 3 colors & 3 neurons 3 objects back-propagation ( Rumelhart, 1986 , LeCun, 1987 ) during 1990s Hochreiter, 1991 , Bengio, 1994 Long short-term memody (LSTM) ( Hochreiter, Schmidhuber, 1997 ) mid-1990s Kernel machines ( Boser, 1992 , Corters, Vapnik, 1995 , Sch\u00f6lkopf, 1999 ) graphical models ( Jordan, 1998 ) LeCun, 1998 , Bengio, 2001 3rd wave of NN deep belief network trained using greedy layer-wise pretraining ( Hinton, 2006 ) Bengio, 2007 , Ranzato, 2007 01.02.02. Increasing Dataset Sizes \u00b6 01.02.03. Increasing Model Sizes \u00b6 01.02.04. Increasing Accuracy, Complexity and Real-World Impact \u00b6 \u00b6 img{width: 51%; float: right;}","title":"01. Introduction"},{"location":"01/#contents","text":"01.00. 01.01. Who Should Read This Book? 01.02. Historical Trends in Deep Learning 01.02.01. The Many Names and Changing Fortunes of Neural Networks 01.02.02. Increasing Dataset Sizes 01.02.03. INcreasing Model Sizes 01.02.04. Increasing Accuracy, Complexity and Real-World Impact","title":"Contents"},{"location":"01/#0100","text":"ancient Greece Galatea by Pygmalion Talos by Daedalus Pandora by Hephaestus AI deep learning logistic regression naive Bayes","title":"01.00"},{"location":"01/#0101_who_should_read_this_book","text":"","title":"01.01. Who Should Read This Book?"},{"location":"01/#0102_historical_trends_in_deep_learning","text":"","title":"01.02. Historical Trends in Deep Learning"},{"location":"01/#010201_the_many_names_and_changing_fortunes_of_neural_networks","text":"cybernetics in 1940s-1960s connectionism in 1980s-1990s deep learning in 2006 artificial neural networks (ANNs) f(x, w) = x_1 x_1 + \\cdots + x_n w_n McCulloch-Pitts Neuron ( McCulloch, Pitts, 1943 ) perceptron ( Rosenblatt, 1958 , 1962 ) ADALINE ( Widrow, Hoff, 1960 ) stochasic gradient descent linear models limitation: connot learn the XOR fn \\begin{cases} f([0,1], w) = 1 \\\\ f([1,0], w) = 1 \\\\ f([1,1], w) = 1 \\\\ f([0,0], w) = 0 \\end{cases} Minsky, Papert, 1969 Olshausen, Field, 2005 ferrets brain ( Von Melshner, 2000 ) Neocognitron ( Fukushima, 1980 ) \u2192 modern convolutional network ( LeCun, 1998 ) rectified linear unit original Cognitron ( Fukushima, 1975 ) Nair, Hinton, 2010 , Glorot, 2011 citing neuroscience, Jarrett, 2009 citing more engineering-oriented 1980s: second wave of NN movement connectionism / parallel distributed processing ( Rumelhart, 1986 , McClelland, 1995 ) models of cognition grounded in neural implementation ( Touretzky, Minton, 1985 ), reviving work og Fonald Hebb in 1940s ( Hebb, 1949 ) connectionsim: large num of sumple computational units can ahchive intelligent behavior distributed representation ( Hinton, 1986 ) e.g., 3 neurons describing 3 colors & 3 neurons 3 objects back-propagation ( Rumelhart, 1986 , LeCun, 1987 ) during 1990s Hochreiter, 1991 , Bengio, 1994 Long short-term memody (LSTM) ( Hochreiter, Schmidhuber, 1997 ) mid-1990s Kernel machines ( Boser, 1992 , Corters, Vapnik, 1995 , Sch\u00f6lkopf, 1999 ) graphical models ( Jordan, 1998 ) LeCun, 1998 , Bengio, 2001 3rd wave of NN deep belief network trained using greedy layer-wise pretraining ( Hinton, 2006 ) Bengio, 2007 , Ranzato, 2007","title":"01.02.01. The Many Names and Changing Fortunes of Neural Networks"},{"location":"01/#010202_increasing_dataset_sizes","text":"","title":"01.02.02. Increasing Dataset Sizes"},{"location":"01/#010203_increasing_model_sizes","text":"","title":"01.02.03. Increasing Model Sizes"},{"location":"01/#010204_increasing_accuracy_complexity_and_real-world_impact","text":"","title":"01.02.04. Increasing Accuracy, Complexity and Real-World Impact"},{"location":"02/","text":"02. Linear Algebra \u00b6 Contents \u00b6 02.00. 02.01. Scalars, Vectors, Matrices and Tensors 02.02. Multiplying Matrices and Vectors 02.03. Identity and Inverse Matrices 02.04. Linear Dependence and Span 02.05. Norms 02.06. Special Kinds of Matricesand Vectors 02.07. Eigendecomposition 02.08. Singular Value Decomposition 02.09. The Moore-Penrose Pseudoinverse 02.10. The Trace Operator 02.11. The Determinant 02.12. Example: Principal Components Analysis 02.00. \u00b6 The Matrix Cookbook ( Peterson, Pedersen, 2006 ) Shilov, 1977 02.01. Scalars, Vectors, Matrices and Tensors \u00b6 scalars vectors: x x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\tag{2.1} matrices: A A = \\begin{bmatrix} A_{1,1} & A_{1,2} \\\\ A_{2,1} & A_{2,2} \\\\ A_{3,1} & A_{3,2} \\end{bmatrix} \\Rightarrow A^T = \\begin{bmatrix} A_{1,1} & A_{2,1} & A_{3,1} \\\\ A_{1,2} & A_{2,2} & A_{3,2} \\end{bmatrix} tensors: \\mathsf{A} A_{i, j, k} def transposition (A^T)_{i,j} = A_{j,i} \\tag{2.3} C = A + B where C_{i,j} = A_{i,j} + B_{i,j} D = a \\cdot B + c where D_{i,j} = a \\cdot B_{i,j} + c C = A + b where C_{i,j} = A_{i,j} + b_j broadacsting 02.02. Multiplying Matrices and Vectors \u00b6 matrix product C = AB \\tag{2.4} C_{i,j} = \\sum_k{A_{i,k} B_{k,j}} \\tag{2.5} element-wise product (aka, Hadamard product ) C = A \\odot B C_{i,j} = A_{i,j} B_{i,j} dot product : x \\cdot y = x^T y x^T y = y^T x \\tag{2.8} (AB)^T = B^T A^T \\tag{2.9} x^T y = (x^T y)^T = y^T x \\tag{2.10} a system of linear equations: Ax = b \\tag{2.11} where A \\in \\mathbb{R}^{m \\times n} , b \\in \\mathbb{R}^m , x \\in \\mathbb{R}^n \\begin{align*} A_{1,:} x &= b_1 \\tag{2.12} \\\\ A_{2,:} x &= b_2 \\tag{2.13} \\\\ \\vdots \\tag{2.14} \\\\ A_{m,:} x &= b_m \\tag{2.15} \\\\ \\end{align*} \\begin{align*} A_{1,1} x_1 + A_{1,2} x_2 + \\cdots + A_{1,n} x_n &= b_1 \\tag{2.16} \\\\ A_{2,1} x_1 + A_{2,2} x_2 + \\cdots + A_{2,n} x_n &= b_2 \\tag{2.17} \\\\ \\vdots \\tag{2.18} \\\\ A_{m,1} x_1 + A_{m,2} x_2 + \\cdots + A_{m,n} x_n &= b_m \\tag{2.19} \\end{align*} 02.03. Identity and Inverse Matrices \u00b6 matrix inversion identity matrix I_n \\in \\mathbb{R}^{n \\times n} \\forall x \\in \\mathbb{R}^n, \\quad I_n x = x \\tag{2.20} A^{-1} A = I_n \\tag{2.21} 02.04. Linear Dependence and Span \u00b6 in order for A^{-1} to exist Ax = b \\tag{2.11} eq.2.11 must have one solution possible to have no solution / infinitely many solutions NOT possible to have more than 1 but less than infinitely solutions if both x and y are solutions then z = \\alpha x + (1 - \\alpha) y \\tag{2.26} is also solution for any real \\alpha Ax = \\sum_i{x_i A_{:,i}} \\tag{2.27} linear combination \\{ v^{(1)}, \\cdots, v^{(n)} \\} \\sum_i{c_i v^{(i)}} \\tag{2.28} span Ax = b have solution for all vals of b \\in \\mathbb{R}^m 02.05. Norms \u00b6 norm : fn that measure size of vectors L^p norm is given by ||x||_p = \\Bigg( \\sum_i{|x_i|^p} \\Bigg)^{\\frac{1}{p}} \\tag{2.30} for p \\in \\mathbb{R} , p \\ge 1 f(x) = 0 \u21d2 x=0 f(x + y) \\le f(x) + f(y) ( triangle inequality ) \\forall\\alpha \\in \\mathbb{R} , f(\\alpha x) = |\\alpha| f(x) L^2 norm, with p=2 ka Euclidean norm ( ||x|| ) L^1 norm ||x||_1 = \\sum_i{|x_i|} \\tag{2.31} L^\\infty norm (aka, max norm ) ||x||_\\infty = \\max_i{|x_i|} \\tag{2.32} Frobenius norm ||A||_F = \\sqrt{\\sum_{i,j}{A_{i,j}^2}} \\tag{2.33} is analogous to L^2 norm of vec x^T y = ||x||_2 ||y||_2 \\cos{\\theta} \\tag{2.34} where \\theta is angle between x , y 02.06. Special Kinds of Matricesand Vectors \u00b6 diagonal D if and only if D_{i,j}=0 for all i \\ne j np.random.seed(123) v = np.random.randint(9, size=5) np.diag(v) array([[2, 0, 0, 0, 0], [0, 2, 0, 0, 0], [0, 0, 6, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 3]]) \\text{diag}(v) x = v \\odot x np.matmul(np.diag(v), x) np.multiply(v, x) \\text{diag}(v)^{-1} = \\text{diag}\\Bigg( \\bigg[\\frac{1}{v_1}, \\ldots, \\frac{1}{v_n} \\bigg]^T \\Bigg) np.linalg.inv(diag_v) np.diag([1/vi for vi in v]) symmetric mat A = A^T \\tag{2.35} unit vector : vec w/ unit norm : ||x||_2 = 1 \\tag{2.36} vec x and vec y orthogonal to each other if x^T y = 0 orthogonal & unit norm orthonormal orthogonal matrix A^T A = A A^T = I \\tag{2.37} implies A^{-1} = A^T \\tag{2.38} from scipy.stats import ortho_group A = ortho_group.rvs(dim = 3) np.set_printoptions(suppress=True) AT_A = np.matmul(A, A.T) 02.07. Eigendecomposition \u00b6 eigendecomposition : into set of eigenvectors & eigenvalues eigenvector of square mat A is non-zero vec v Av = \\lambda v \\tag{2.39} eigenvalue \\lambda l, V = np.linalg.eig(A) mat A has n linearly independent eigenvec \\{ v^{(1)}, \\cdots, v^{(n)} \\} , w/ corresponding eigenval \\{ \\lambda_1, \\cdots, \\lambda_n \\} mat V : V = [ v^{(1)}, \\cdots, v^{(b)} ] , vec \\lambda : [\\lambda_1, \\cdots, \\lambda_n]^T eigendecomposition A = V\\text{diag}(\\lambda) V^{-1} \\tag{2.40} np.linalg.multi_dot([V, np.diag(l), np.linalg.inv(V)]) \\begin{align*} A &= Q \\Lambda Q^T \\\\ &= \\begin{bmatrix} v_1 & v_2 & \\cdots & v_n \\end{bmatrix} \\begin{bmatrix} \\lambda_1 & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\lambda_n \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} \\end{align*} \\tag{2.41} eigenvectors, orthogonal mat Q of A , eigenvalues, diagonal mat \\Lambda of A eigenvalue \\Lambda_{i,i} is associated w eigenvector Q_{:,i} f(x) = x^T Ax \\quad (||x|| = 1) positive definite : mat whose eigenval are all positive positive semidefinite : eigenvals are all positive / zero negative definite : eigenvals all negative negative semidefinite : eigenvals negative / zero positive semideginite mat \\forall x, x^T Ax \\ge 0 positive definite mat x^T Ax = 0 \\Rightarrow x = 0 02.08. Singular Value Decomposition \u00b6 singular value decomposition (SVD) singular vecs , singular vals u, s, vh = np.linalg.svd(A, full_matrices=False) eigendecomposition A = V \\text{diag}(\\lambda) V^{-1} \\tag{2.42} singular value decomposition A = UDV^T \\tag{2.43} A m \\times n mat left-singular vec U m \\times m orthogonal mat singular values D m \\times n diagonal mat right-singular vec V n \\times n orthogonal mat 02.09. The Moore-Penrose Pseudoinverse \u00b6 \\begin{align*} Ax &= y \\tag{2.44} \\\\ x &= By \\tag{2.45} \\end{align*} left-inverse B of mat A Moore-Penrose pseudoinverse pseudoinverse of A is defined as A^+ = \\lim\\limits_{\\alpha \\to 0} (A^T A + \\alpha I)^{-1} A^T \\tag{2.46} A^+ = V D^+ U^+ \\tag{2.47} ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ 02.10. The Trace Operator \u00b6 \\mathrm{Tr}(A) = \\sum_i{A_{i,i}} \\tag{2.48} Frobenius norm ||A||_F = \\sqrt{\\mathrm{Tr}(A A^T)} \\tag{2.49} \\mathrm{Tr}(A) = \\mathrm{Tr}(A^T) \\tag{2.50} \\mathrm{Tr}(ABC) = \\mathrm{Tr}(CAB) = \\mathrm{Tr}(BCA) \\tag{2.51} more generally \\mathrm{Tr}\\bigg(\\prod_{i=1}^n{F^{(i)}}\\bigg) = \\mathrm{Tr}\\bigg( F^{(n)} \\prod_{i=1}^{n-1}{F^{(i)}} \\bigg) \\tag{2.52} A \\in \\mathbb{R}^{m \\times n} , B \\in \\mathbb{R}^{n \\times m} \\mathrm{Tr}(AB) = \\mathrm{Tr}(BA) \\tag{2.53} though AB \\in \\mathbb{R}^{m \\times m} , BA \\in \\mathbb{R}^{n \\times n} 02.11. The Determinant \u00b6 \\det{(A)} 02.12. Example: Principal Components Analysis \u00b6 principal components analysis (PCA) m points \\{ x^{(1)}, \\ldots, x^{(m)} \\} in \\mathbb{R}^n lower-dim version corresponding code vec c^{(i)} \\in \\mathbb{R}^l l < n encoding fn f(x) = c decoding fn x \\approx g(f(x)) map code back into \\mathbb{R}^n let g(c) = Dc , where D \\in \\mathbb{R}^{n \\times l} L^2 norm: c^* \\arg_c\\min{||x - g(c)||_2} \\tag{2.54} c^* = \\arg_c\\min{||x - g(c)||_2^2} \\tag{2.55} \\begin{align*} &(x - g(c))^T (x - g(c)) \\tag{2.56} \\\\ &= x^T x - x^T g(c) - g(c)^T x + g(c)^T g(c) \\tag{2.57} \\\\ &= x^T x - 2x^T g(c) + g(c)^T g(c) \\tag{2.58} \\end{align*} ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ \u00b6 img{width: 51%; float: right;}","title":"02. Linear Algebra"},{"location":"02/#contents","text":"02.00. 02.01. Scalars, Vectors, Matrices and Tensors 02.02. Multiplying Matrices and Vectors 02.03. Identity and Inverse Matrices 02.04. Linear Dependence and Span 02.05. Norms 02.06. Special Kinds of Matricesand Vectors 02.07. Eigendecomposition 02.08. Singular Value Decomposition 02.09. The Moore-Penrose Pseudoinverse 02.10. The Trace Operator 02.11. The Determinant 02.12. Example: Principal Components Analysis","title":"Contents"},{"location":"02/#0200","text":"The Matrix Cookbook ( Peterson, Pedersen, 2006 ) Shilov, 1977","title":"02.00."},{"location":"02/#0201_scalars_vectors_matrices_and_tensors","text":"scalars vectors: x x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\tag{2.1} matrices: A A = \\begin{bmatrix} A_{1,1} & A_{1,2} \\\\ A_{2,1} & A_{2,2} \\\\ A_{3,1} & A_{3,2} \\end{bmatrix} \\Rightarrow A^T = \\begin{bmatrix} A_{1,1} & A_{2,1} & A_{3,1} \\\\ A_{1,2} & A_{2,2} & A_{3,2} \\end{bmatrix} tensors: \\mathsf{A} A_{i, j, k} def transposition (A^T)_{i,j} = A_{j,i} \\tag{2.3} C = A + B where C_{i,j} = A_{i,j} + B_{i,j} D = a \\cdot B + c where D_{i,j} = a \\cdot B_{i,j} + c C = A + b where C_{i,j} = A_{i,j} + b_j broadacsting","title":"02.01. Scalars, Vectors, Matrices and Tensors"},{"location":"02/#0202_multiplying_matrices_and_vectors","text":"matrix product C = AB \\tag{2.4} C_{i,j} = \\sum_k{A_{i,k} B_{k,j}} \\tag{2.5} element-wise product (aka, Hadamard product ) C = A \\odot B C_{i,j} = A_{i,j} B_{i,j} dot product : x \\cdot y = x^T y x^T y = y^T x \\tag{2.8} (AB)^T = B^T A^T \\tag{2.9} x^T y = (x^T y)^T = y^T x \\tag{2.10} a system of linear equations: Ax = b \\tag{2.11} where A \\in \\mathbb{R}^{m \\times n} , b \\in \\mathbb{R}^m , x \\in \\mathbb{R}^n \\begin{align*} A_{1,:} x &= b_1 \\tag{2.12} \\\\ A_{2,:} x &= b_2 \\tag{2.13} \\\\ \\vdots \\tag{2.14} \\\\ A_{m,:} x &= b_m \\tag{2.15} \\\\ \\end{align*} \\begin{align*} A_{1,1} x_1 + A_{1,2} x_2 + \\cdots + A_{1,n} x_n &= b_1 \\tag{2.16} \\\\ A_{2,1} x_1 + A_{2,2} x_2 + \\cdots + A_{2,n} x_n &= b_2 \\tag{2.17} \\\\ \\vdots \\tag{2.18} \\\\ A_{m,1} x_1 + A_{m,2} x_2 + \\cdots + A_{m,n} x_n &= b_m \\tag{2.19} \\end{align*}","title":"02.02. Multiplying Matrices and Vectors"},{"location":"02/#0203_identity_and_inverse_matrices","text":"matrix inversion identity matrix I_n \\in \\mathbb{R}^{n \\times n} \\forall x \\in \\mathbb{R}^n, \\quad I_n x = x \\tag{2.20} A^{-1} A = I_n \\tag{2.21}","title":"02.03. Identity and Inverse Matrices"},{"location":"02/#0204_linear_dependence_and_span","text":"in order for A^{-1} to exist Ax = b \\tag{2.11} eq.2.11 must have one solution possible to have no solution / infinitely many solutions NOT possible to have more than 1 but less than infinitely solutions if both x and y are solutions then z = \\alpha x + (1 - \\alpha) y \\tag{2.26} is also solution for any real \\alpha Ax = \\sum_i{x_i A_{:,i}} \\tag{2.27} linear combination \\{ v^{(1)}, \\cdots, v^{(n)} \\} \\sum_i{c_i v^{(i)}} \\tag{2.28} span Ax = b have solution for all vals of b \\in \\mathbb{R}^m","title":"02.04. Linear Dependence and Span"},{"location":"02/#0205_norms","text":"norm : fn that measure size of vectors L^p norm is given by ||x||_p = \\Bigg( \\sum_i{|x_i|^p} \\Bigg)^{\\frac{1}{p}} \\tag{2.30} for p \\in \\mathbb{R} , p \\ge 1 f(x) = 0 \u21d2 x=0 f(x + y) \\le f(x) + f(y) ( triangle inequality ) \\forall\\alpha \\in \\mathbb{R} , f(\\alpha x) = |\\alpha| f(x) L^2 norm, with p=2 ka Euclidean norm ( ||x|| ) L^1 norm ||x||_1 = \\sum_i{|x_i|} \\tag{2.31} L^\\infty norm (aka, max norm ) ||x||_\\infty = \\max_i{|x_i|} \\tag{2.32} Frobenius norm ||A||_F = \\sqrt{\\sum_{i,j}{A_{i,j}^2}} \\tag{2.33} is analogous to L^2 norm of vec x^T y = ||x||_2 ||y||_2 \\cos{\\theta} \\tag{2.34} where \\theta is angle between x , y","title":"02.05. Norms"},{"location":"02/#0206_special_kinds_of_matricesand_vectors","text":"diagonal D if and only if D_{i,j}=0 for all i \\ne j np.random.seed(123) v = np.random.randint(9, size=5) np.diag(v) array([[2, 0, 0, 0, 0], [0, 2, 0, 0, 0], [0, 0, 6, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 3]]) \\text{diag}(v) x = v \\odot x np.matmul(np.diag(v), x) np.multiply(v, x) \\text{diag}(v)^{-1} = \\text{diag}\\Bigg( \\bigg[\\frac{1}{v_1}, \\ldots, \\frac{1}{v_n} \\bigg]^T \\Bigg) np.linalg.inv(diag_v) np.diag([1/vi for vi in v]) symmetric mat A = A^T \\tag{2.35} unit vector : vec w/ unit norm : ||x||_2 = 1 \\tag{2.36} vec x and vec y orthogonal to each other if x^T y = 0 orthogonal & unit norm orthonormal orthogonal matrix A^T A = A A^T = I \\tag{2.37} implies A^{-1} = A^T \\tag{2.38} from scipy.stats import ortho_group A = ortho_group.rvs(dim = 3) np.set_printoptions(suppress=True) AT_A = np.matmul(A, A.T)","title":"02.06. Special Kinds of Matricesand Vectors"},{"location":"02/#0207_eigendecomposition","text":"eigendecomposition : into set of eigenvectors & eigenvalues eigenvector of square mat A is non-zero vec v Av = \\lambda v \\tag{2.39} eigenvalue \\lambda l, V = np.linalg.eig(A) mat A has n linearly independent eigenvec \\{ v^{(1)}, \\cdots, v^{(n)} \\} , w/ corresponding eigenval \\{ \\lambda_1, \\cdots, \\lambda_n \\} mat V : V = [ v^{(1)}, \\cdots, v^{(b)} ] , vec \\lambda : [\\lambda_1, \\cdots, \\lambda_n]^T eigendecomposition A = V\\text{diag}(\\lambda) V^{-1} \\tag{2.40} np.linalg.multi_dot([V, np.diag(l), np.linalg.inv(V)]) \\begin{align*} A &= Q \\Lambda Q^T \\\\ &= \\begin{bmatrix} v_1 & v_2 & \\cdots & v_n \\end{bmatrix} \\begin{bmatrix} \\lambda_1 & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\lambda_n \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} \\end{align*} \\tag{2.41} eigenvectors, orthogonal mat Q of A , eigenvalues, diagonal mat \\Lambda of A eigenvalue \\Lambda_{i,i} is associated w eigenvector Q_{:,i} f(x) = x^T Ax \\quad (||x|| = 1) positive definite : mat whose eigenval are all positive positive semidefinite : eigenvals are all positive / zero negative definite : eigenvals all negative negative semidefinite : eigenvals negative / zero positive semideginite mat \\forall x, x^T Ax \\ge 0 positive definite mat x^T Ax = 0 \\Rightarrow x = 0","title":"02.07. Eigendecomposition"},{"location":"02/#0208_singular_value_decomposition","text":"singular value decomposition (SVD) singular vecs , singular vals u, s, vh = np.linalg.svd(A, full_matrices=False) eigendecomposition A = V \\text{diag}(\\lambda) V^{-1} \\tag{2.42} singular value decomposition A = UDV^T \\tag{2.43} A m \\times n mat left-singular vec U m \\times m orthogonal mat singular values D m \\times n diagonal mat right-singular vec V n \\times n orthogonal mat","title":"02.08. Singular Value Decomposition"},{"location":"02/#0209_the_moore-penrose_pseudoinverse","text":"\\begin{align*} Ax &= y \\tag{2.44} \\\\ x &= By \\tag{2.45} \\end{align*} left-inverse B of mat A Moore-Penrose pseudoinverse pseudoinverse of A is defined as A^+ = \\lim\\limits_{\\alpha \\to 0} (A^T A + \\alpha I)^{-1} A^T \\tag{2.46} A^+ = V D^+ U^+ \\tag{2.47} ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~","title":"02.09. The Moore-Penrose Pseudoinverse"},{"location":"02/#0210_the_trace_operator","text":"\\mathrm{Tr}(A) = \\sum_i{A_{i,i}} \\tag{2.48} Frobenius norm ||A||_F = \\sqrt{\\mathrm{Tr}(A A^T)} \\tag{2.49} \\mathrm{Tr}(A) = \\mathrm{Tr}(A^T) \\tag{2.50} \\mathrm{Tr}(ABC) = \\mathrm{Tr}(CAB) = \\mathrm{Tr}(BCA) \\tag{2.51} more generally \\mathrm{Tr}\\bigg(\\prod_{i=1}^n{F^{(i)}}\\bigg) = \\mathrm{Tr}\\bigg( F^{(n)} \\prod_{i=1}^{n-1}{F^{(i)}} \\bigg) \\tag{2.52} A \\in \\mathbb{R}^{m \\times n} , B \\in \\mathbb{R}^{n \\times m} \\mathrm{Tr}(AB) = \\mathrm{Tr}(BA) \\tag{2.53} though AB \\in \\mathbb{R}^{m \\times m} , BA \\in \\mathbb{R}^{n \\times n}","title":"02.10. The Trace Operator"},{"location":"02/#0211_the_determinant","text":"\\det{(A)}","title":"02.11. The Determinant"},{"location":"02/#0212_example_principal_components_analysis","text":"principal components analysis (PCA) m points \\{ x^{(1)}, \\ldots, x^{(m)} \\} in \\mathbb{R}^n lower-dim version corresponding code vec c^{(i)} \\in \\mathbb{R}^l l < n encoding fn f(x) = c decoding fn x \\approx g(f(x)) map code back into \\mathbb{R}^n let g(c) = Dc , where D \\in \\mathbb{R}^{n \\times l} L^2 norm: c^* \\arg_c\\min{||x - g(c)||_2} \\tag{2.54} c^* = \\arg_c\\min{||x - g(c)||_2^2} \\tag{2.55} \\begin{align*} &(x - g(c))^T (x - g(c)) \\tag{2.56} \\\\ &= x^T x - x^T g(c) - g(c)^T x + g(c)^T g(c) \\tag{2.57} \\\\ &= x^T x - 2x^T g(c) + g(c)^T g(c) \\tag{2.58} \\end{align*} ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~","title":"02.12. Example: Principal Components Analysis"},{"location":"03/","text":"03. Probability and Information Theory \u00b6 Contents \u00b6 03.00. 03.01. Why Probability? 03.02. Random Variables 03.03. Probability Distributions 03.03.01. Disctrete Variables and Probability Mass Functions 03.03.02. Continuous Variables and Probability Density Functions 03.04. Marginal Probability 03.05. Conditional Probability 03.06. The Chain Rule of Conditional Probabilities 03.07. Independence and Conditional Independence 03.08. Expectation, Variance and Covariance 03.09. Common Probability Distributions 03.10. Useful Properties of Common Functions 03.11. Bayes\u2019 Rule 03.12. Technical Details of Continuous Variables 03.13. Information Theory 03.14. Structured Probabilistic Models 03.00. \u00b6 Jaynes (2003) 03.01. Why Probability? \u00b6 frequent prob Bayesian prob Ramsey (1926) 03.02. Random Variables \u00b6 random var 03.03. Probability Distributions \u00b6 probability dist 03.03.01. Disctrete Variables and Probability Mass Functions \u00b6 probability mass fn (PMF) P(x) define var, specify which dist it follows \\mathrm{x} \\sim P(\\mathrm{x}) joint prob dist P(\\mathrm{x} = x, \\mathrm{y} = y) \\forall x \\in \\mathrm{x}, 0 \\le P(x) \\le 1 \\sum_{x \\in \\mathrm{x}}{P(x)} = 1 , normalized uniform dist P(\\mathrm{x} = x_i) = \\frac{1}{k} \\tag{3.1} \\sum_i{P(\\mathrm{x} = x_i)} = \\sum_i{\\frac{1}{k}} = \\frac{k}{k} = 1 \\tag{3.2} 03.03.02. Continuous Variables and Probability Density Functions \u00b6 probability density fn (PDF) p \\forall x \\in \\mathrm{x}, p(x) \\ge 0 NOT require p(x) \\le 1 \\int{p(x) dx} = 1 03.04. Marginal Probability \u00b6 03.05. Conditional Probability \u00b6 03.06. The Chain Rule of Conditional Probabilities \u00b6 03.07. Independence and Conditional Independence \u00b6 03.08. Expectation, Variance and Covariance \u00b6 03.09. Common Probability Distributions \u00b6 03.10. Useful Properties of Common Functions \u00b6 03.11. Bayes\u2019 Rule \u00b6 03.12. Technical Details of Continuous Variables \u00b6 03.13. Information Theory \u00b6 03.14. Structured Probabilistic Models \u00b6 \u00b6 img{width: 51%; float: right;}","title":"03. Probability and Information Theory"},{"location":"03/#contents","text":"03.00. 03.01. Why Probability? 03.02. Random Variables 03.03. Probability Distributions 03.03.01. Disctrete Variables and Probability Mass Functions 03.03.02. Continuous Variables and Probability Density Functions 03.04. Marginal Probability 03.05. Conditional Probability 03.06. The Chain Rule of Conditional Probabilities 03.07. Independence and Conditional Independence 03.08. Expectation, Variance and Covariance 03.09. Common Probability Distributions 03.10. Useful Properties of Common Functions 03.11. Bayes\u2019 Rule 03.12. Technical Details of Continuous Variables 03.13. Information Theory 03.14. Structured Probabilistic Models","title":"Contents"},{"location":"03/#0300","text":"Jaynes (2003)","title":"03.00."},{"location":"03/#0301_why_probability","text":"frequent prob Bayesian prob Ramsey (1926)","title":"03.01. Why Probability?"},{"location":"03/#0302_random_variables","text":"random var","title":"03.02. Random Variables"},{"location":"03/#0303_probability_distributions","text":"probability dist","title":"03.03. Probability Distributions"},{"location":"03/#030301_disctrete_variables_and_probability_mass_functions","text":"probability mass fn (PMF) P(x) define var, specify which dist it follows \\mathrm{x} \\sim P(\\mathrm{x}) joint prob dist P(\\mathrm{x} = x, \\mathrm{y} = y) \\forall x \\in \\mathrm{x}, 0 \\le P(x) \\le 1 \\sum_{x \\in \\mathrm{x}}{P(x)} = 1 , normalized uniform dist P(\\mathrm{x} = x_i) = \\frac{1}{k} \\tag{3.1} \\sum_i{P(\\mathrm{x} = x_i)} = \\sum_i{\\frac{1}{k}} = \\frac{k}{k} = 1 \\tag{3.2}","title":"03.03.01. Disctrete Variables and Probability Mass Functions"},{"location":"03/#030302_continuous_variables_and_probability_density_functions","text":"probability density fn (PDF) p \\forall x \\in \\mathrm{x}, p(x) \\ge 0 NOT require p(x) \\le 1 \\int{p(x) dx} = 1","title":"03.03.02. Continuous Variables and Probability Density Functions"},{"location":"03/#0304_marginal_probability","text":"","title":"03.04. Marginal Probability"},{"location":"03/#0305_conditional_probability","text":"","title":"03.05. Conditional Probability"},{"location":"03/#0306_the_chain_rule_of_conditional_probabilities","text":"","title":"03.06. The Chain Rule of Conditional Probabilities"},{"location":"03/#0307_independence_and_conditional_independence","text":"","title":"03.07. Independence and Conditional Independence"},{"location":"03/#0308_expectation_variance_and_covariance","text":"","title":"03.08. Expectation, Variance and Covariance"},{"location":"03/#0309_common_probability_distributions","text":"","title":"03.09. Common Probability Distributions"},{"location":"03/#0310_useful_properties_of_common_functions","text":"","title":"03.10. Useful Properties of Common Functions"},{"location":"03/#0311_bayes_rule","text":"","title":"03.11. Bayes\u2019 Rule"},{"location":"03/#0312_technical_details_of_continuous_variables","text":"","title":"03.12. Technical Details of Continuous Variables"},{"location":"03/#0313_information_theory","text":"","title":"03.13. Information Theory"},{"location":"03/#0314_structured_probabilistic_models","text":"","title":"03.14. Structured Probabilistic Models"},{"location":"04/","text":"04. Numerical Computation \u00b6 Contents \u00b6 \u00b6 \u00b6 img{width: 51%; float: right;}","title":"04. Numerical Computation"},{"location":"04/#contents","text":"","title":"Contents"},{"location":"05/","text":"05. Machine Learning Basics \u00b6 Contents \u00b6 05.00. 05.01. Learning Algorithms 05.01.01. The Task, T 05.01.02. The Performance Measure, P 05.01.03. The Experience, E 05.01.04. Example: Linear Regression 05.02. Capacity, Overfitting and Underfitting 05.02.01. The No Free Lunch Theorem 05.02.02. Regularization 05.03. Hyperparameters and Validation Sets 05.03.01. Cross-Validation 05.04. Estimators, Bias and Variance 05.04.01. Point Estimation 05.04.02. Bias 05.04.03. Variance and Standard Error 05.04.04. Trading off Bias and Variance to Minimize Mean Squared Error [05.04.05. Consistency][050405] [05.05. Maximum Likelihood Estimation][0505] [05.05.01. Conditional Log-Likelihood and Mean Squared Error][050501] [05.05.02. Properties of Maximum Likelihood][050502] [05.06. Bayesian Statistics][0506] [05.06.01. Maximum A Posteori (MAP) Estimation][050601] [05.07. Supervised Learning Algorithm][0507] [05.07.01. Probabilistic Supervised Learning][050701] [05.07.02. Support Vector Machines][050702] [05.07.03. Other Simple Supervised Learning Algorithms][050703] [05.08. Unsupervised Learning Algorithms][0508] [05.08.01. Principal Components Analysis][050801] [05.08.02. k -means Clustering][050802] [05.09. Stochastic Gradient Descent][0509] [05.10. Building a Mchine Learning Algorithm][0510] [05.11. Challenges Motivating Deep Learning][0511] [05.11.01. The Curse of Dimensionality][051101] [05.11.02. Local Constancy and Smoothness Regularization][051102] [05.11.03. Monifold Learning][051103] 05.00. \u00b6 Murphy (2012) , Bishop (2006) 05.01. Learning Algorithms \u00b6 learn from experience E w respect to tasks T & performance measure P 05.01.01. The Task, T \u00b6 classification classification w missing inputs regression transcription machine translation structured output anomaly detection synthesis and samplint mputation of missing values denoising density estimation / probability mass function estimation 05.01.02. The Performance Measure, P \u00b6 accuracy error rate 05.01.03. The Experience, E \u00b6 unsuprevised supervised p(\\mathrm{x}) = \\prod_{i=1}^n{p(\\mathrm{x}_i | \\mathrm{x}_1, \\cdots, \\mathrm{x}_{i-1})} \\tag{5.1} p(y | \\mathrm{x}) = \\frac{p(\\mathrm{x}, y)}{\\sum_{y'}{p(\\mathrm{x}, y')}} \\tag{5.2} reinforcement learning 05.01.04. Example: Linear Regression \u00b6 \\hat{y} = w^T x \\tag{5.3} w \\in \\mathbb{R}^n mean squared error \\text{MSE}_{\\text{text}} = \\frac{1}{m} \\sum_i{( \\hat{y}^{\\text{(test)}} - y^{(\\text{test})} )_i^2} \\tag{5.4} \\text{MSE}_{\\text{text}} = \\frac{1}{m} \\Big\\| \\hat{y}^{\\text{(test)}} - y^{(\\text{test})} \\Big\\|_2^2 \\tag{5.4} to minimize \\text{MSE}_{\\text{train}} , gradient \\begin{align*} \\nabla_w \\text{MSE}_{\\text{train}} &= 0 \\tag{5.6} \\\\ \\nabla_w \\frac{1}{m} \\Big\\| \\hat{y}^{\\text{train}} - y^{\\text{(train)}} \\Big|_2^2 &= 0 \\tag{5.7} \\\\ \\frac{1}{m} \\nabla_w \\Big\\| X^{(\\text{train})} w - y^{\\text{(train)}} \\Big\\|_2^2 &= 0 \\tag{5.8} \\end{align*} \\begin{align*} \\nabla_w \\Big( X^{\\text{train}} w - y^{\\text{train}} \\Big)^T \\Big( X^{\\text{train}} w - y^{\\text{train}} \\Big) &= 0 \\tag{5.9} \\\\ w X^{\\text{(train)} T} X^{\\text{(train)}} w - 2X^{\\text{(train)} T} y^{\\text{(train)}} &= 0 \\tag{5.11} \\\\ w = \\Big( X^{\\text{(train)} T} X^{\\text{(train)}} \\Big)^{-1} X^{\\text{(train)} T} y^{\\text{(train)}} \\tag{5.12} \\end{align*} eq.5.12 normal eq \\hat{y} = w^T x + b \\tag{5.13} 05.02. Capacity, Overfitting and Underfitting \u00b6 05.02.01. The No Free Lunch Theorem \u00b6 05.02.02. Regularization \u00b6 05.03. Hyperparameters and Validation Sets \u00b6 05.03.01. Cross-Validation \u00b6 05.04. Estimators, Bias and Variance \u00b6 05.04.01. Point Estimation \u00b6 05.04.02. Bias \u00b6 05.04.03. Variance and Standard Error \u00b6 05.04.04. Trading off Bias and Variance to Minimize Mean Squared Error \u00b6 05.04.05. Consistency \u00b6 05.05. Maximum Likelihood Estimation \u00b6 05.05.01. Conditional Log-Likelihood and Mean Squared Error \u00b6 05.05.02. Properties of Maximum Likelihood \u00b6 05.06. Bayesian Statistics \u00b6 05.06.01. Maximum A Posteori (MAP) Estimation \u00b6 05.07. Supervised Learning Algorithm \u00b6 05.07.01. Probabilistic Supervised Learning \u00b6 05.07.02. Support Vector Machines \u00b6 05.07.03. Other Simple Supervised Learning Algorithms \u00b6 05.08. Unsupervised Learning Algorithms \u00b6 05.08.01. Principal Components Analysis \u00b6 05.08.02. k -means Clustering \u00b6 05.09. Stochastic Gradient Descent \u00b6 05.10. Building a Mchine Learning Algorithm \u00b6 05.11. Challenges Motivating Deep Learning \u00b6 05.11.01. The Curse of Dimensionality \u00b6 05.11.02. Local Constancy and Smoothness Regularization \u00b6 05.11.03. Monifold Learning \u00b6 \u00b6 img{width: 51%; float: right;}","title":"05. Machine Learning Basics"},{"location":"05/#contents","text":"05.00. 05.01. Learning Algorithms 05.01.01. The Task, T 05.01.02. The Performance Measure, P 05.01.03. The Experience, E 05.01.04. Example: Linear Regression 05.02. Capacity, Overfitting and Underfitting 05.02.01. The No Free Lunch Theorem 05.02.02. Regularization 05.03. Hyperparameters and Validation Sets 05.03.01. Cross-Validation 05.04. Estimators, Bias and Variance 05.04.01. Point Estimation 05.04.02. Bias 05.04.03. Variance and Standard Error 05.04.04. Trading off Bias and Variance to Minimize Mean Squared Error [05.04.05. Consistency][050405] [05.05. Maximum Likelihood Estimation][0505] [05.05.01. Conditional Log-Likelihood and Mean Squared Error][050501] [05.05.02. Properties of Maximum Likelihood][050502] [05.06. Bayesian Statistics][0506] [05.06.01. Maximum A Posteori (MAP) Estimation][050601] [05.07. Supervised Learning Algorithm][0507] [05.07.01. Probabilistic Supervised Learning][050701] [05.07.02. Support Vector Machines][050702] [05.07.03. Other Simple Supervised Learning Algorithms][050703] [05.08. Unsupervised Learning Algorithms][0508] [05.08.01. Principal Components Analysis][050801] [05.08.02. k -means Clustering][050802] [05.09. Stochastic Gradient Descent][0509] [05.10. Building a Mchine Learning Algorithm][0510] [05.11. Challenges Motivating Deep Learning][0511] [05.11.01. The Curse of Dimensionality][051101] [05.11.02. Local Constancy and Smoothness Regularization][051102] [05.11.03. Monifold Learning][051103]","title":"Contents"},{"location":"05/#0500","text":"Murphy (2012) , Bishop (2006)","title":"05.00."},{"location":"05/#0501_learning_algorithms","text":"learn from experience E w respect to tasks T & performance measure P","title":"05.01. Learning Algorithms"},{"location":"05/#050101_the_task_t","text":"classification classification w missing inputs regression transcription machine translation structured output anomaly detection synthesis and samplint mputation of missing values denoising density estimation / probability mass function estimation","title":"05.01.01. The Task, T"},{"location":"05/#050102_the_performance_measure_p","text":"accuracy error rate","title":"05.01.02. The Performance Measure, P"},{"location":"05/#050103_the_experience_e","text":"unsuprevised supervised p(\\mathrm{x}) = \\prod_{i=1}^n{p(\\mathrm{x}_i | \\mathrm{x}_1, \\cdots, \\mathrm{x}_{i-1})} \\tag{5.1} p(y | \\mathrm{x}) = \\frac{p(\\mathrm{x}, y)}{\\sum_{y'}{p(\\mathrm{x}, y')}} \\tag{5.2} reinforcement learning","title":"05.01.03. The Experience, E"},{"location":"05/#050104_example_linear_regression","text":"\\hat{y} = w^T x \\tag{5.3} w \\in \\mathbb{R}^n mean squared error \\text{MSE}_{\\text{text}} = \\frac{1}{m} \\sum_i{( \\hat{y}^{\\text{(test)}} - y^{(\\text{test})} )_i^2} \\tag{5.4} \\text{MSE}_{\\text{text}} = \\frac{1}{m} \\Big\\| \\hat{y}^{\\text{(test)}} - y^{(\\text{test})} \\Big\\|_2^2 \\tag{5.4} to minimize \\text{MSE}_{\\text{train}} , gradient \\begin{align*} \\nabla_w \\text{MSE}_{\\text{train}} &= 0 \\tag{5.6} \\\\ \\nabla_w \\frac{1}{m} \\Big\\| \\hat{y}^{\\text{train}} - y^{\\text{(train)}} \\Big|_2^2 &= 0 \\tag{5.7} \\\\ \\frac{1}{m} \\nabla_w \\Big\\| X^{(\\text{train})} w - y^{\\text{(train)}} \\Big\\|_2^2 &= 0 \\tag{5.8} \\end{align*} \\begin{align*} \\nabla_w \\Big( X^{\\text{train}} w - y^{\\text{train}} \\Big)^T \\Big( X^{\\text{train}} w - y^{\\text{train}} \\Big) &= 0 \\tag{5.9} \\\\ w X^{\\text{(train)} T} X^{\\text{(train)}} w - 2X^{\\text{(train)} T} y^{\\text{(train)}} &= 0 \\tag{5.11} \\\\ w = \\Big( X^{\\text{(train)} T} X^{\\text{(train)}} \\Big)^{-1} X^{\\text{(train)} T} y^{\\text{(train)}} \\tag{5.12} \\end{align*} eq.5.12 normal eq \\hat{y} = w^T x + b \\tag{5.13}","title":"05.01.04. Example: Linear Regression"},{"location":"05/#0502_capacity_overfitting_and_underfitting","text":"","title":"05.02. Capacity, Overfitting and Underfitting"},{"location":"05/#050201_the_no_free_lunch_theorem","text":"","title":"05.02.01. The No Free Lunch Theorem"},{"location":"05/#050202_regularization","text":"","title":"05.02.02. Regularization"},{"location":"05/#0503_hyperparameters_and_validation_sets","text":"","title":"05.03. Hyperparameters and Validation Sets"},{"location":"05/#050301_cross-validation","text":"","title":"05.03.01. Cross-Validation"},{"location":"05/#0504_estimators_bias_and_variance","text":"","title":"05.04. Estimators, Bias and Variance"},{"location":"05/#050401_point_estimation","text":"","title":"05.04.01. Point Estimation"},{"location":"05/#050402_bias","text":"","title":"05.04.02. Bias"},{"location":"05/#050403_variance_and_standard_error","text":"","title":"05.04.03. Variance and Standard Error"},{"location":"05/#050404_trading_off_bias_and_variance_to_minimize_mean_squared_error","text":"","title":"05.04.04. Trading off Bias and Variance to Minimize Mean Squared Error"},{"location":"05/#050405_consistency","text":"","title":"05.04.05. Consistency"},{"location":"05/#0505_maximum_likelihood_estimation","text":"","title":"05.05. Maximum Likelihood Estimation"},{"location":"05/#050501_conditional_log-likelihood_and_mean_squared_error","text":"","title":"05.05.01. Conditional Log-Likelihood and Mean Squared Error"},{"location":"05/#050502_properties_of_maximum_likelihood","text":"","title":"05.05.02. Properties of Maximum Likelihood"},{"location":"05/#0506_bayesian_statistics","text":"","title":"05.06. Bayesian Statistics"},{"location":"05/#050601_maximum_a_posteori_map_estimation","text":"","title":"05.06.01. Maximum A Posteori (MAP) Estimation"},{"location":"05/#0507_supervised_learning_algorithm","text":"","title":"05.07. Supervised Learning Algorithm"},{"location":"05/#050701_probabilistic_supervised_learning","text":"","title":"05.07.01. Probabilistic Supervised Learning"},{"location":"05/#050702_support_vector_machines","text":"","title":"05.07.02. Support Vector Machines"},{"location":"05/#050703_other_simple_supervised_learning_algorithms","text":"","title":"05.07.03. Other Simple Supervised Learning Algorithms"},{"location":"05/#0508_unsupervised_learning_algorithms","text":"","title":"05.08. Unsupervised Learning Algorithms"},{"location":"05/#050801_principal_components_analysis","text":"","title":"05.08.01. Principal Components Analysis"},{"location":"05/#050802_k-means_clustering","text":"","title":"05.08.02. k-means Clustering"},{"location":"05/#0509_stochastic_gradient_descent","text":"","title":"05.09. Stochastic Gradient Descent"},{"location":"05/#0510_building_a_mchine_learning_algorithm","text":"","title":"05.10. Building a Mchine Learning Algorithm"},{"location":"05/#0511_challenges_motivating_deep_learning","text":"","title":"05.11. Challenges Motivating Deep Learning"},{"location":"05/#051101_the_curse_of_dimensionality","text":"","title":"05.11.01. The Curse of Dimensionality"},{"location":"05/#051102_local_constancy_and_smoothness_regularization","text":"","title":"05.11.02. Local Constancy and Smoothness Regularization"},{"location":"05/#051103_monifold_learning","text":"","title":"05.11.03. Monifold Learning"}]}
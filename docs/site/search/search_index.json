{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Deep Learning Book \u00b6 Contents \u00b6 Part 0. 01. Introduction Part I. Applied Math and Machine Learning Basics 02. Linear Algebra 03. Probability and Information Theory 04. Numerical Computation 05. Machine Learning Basics Part II. Deep Networks: Modern Practices 06. Deep Feedforward Networks 07. Regularization for Deep Learning 08. Optimization for Training Deep Models 09. Convolutional Networks 10. Sequence Modeling: Recurrent and Recursive Nets 11. Practical Methodology 12. Applications Part III. Deep Learning Research 13. Linear Factor Models 14. Autoencoders 15. Representation Learning 16. Structured Probabilistic Models for Deep Learning 17. Monte Carlo Methods 18. Confronting the Partition Function 19. Approximate Inference 20. Deep Generative Models \u00b6 img{width: 51%; float: right;}","title":"Home"},{"location":"#contents","text":"Part 0. 01. Introduction Part I. Applied Math and Machine Learning Basics 02. Linear Algebra 03. Probability and Information Theory 04. Numerical Computation 05. Machine Learning Basics Part II. Deep Networks: Modern Practices 06. Deep Feedforward Networks 07. Regularization for Deep Learning 08. Optimization for Training Deep Models 09. Convolutional Networks 10. Sequence Modeling: Recurrent and Recursive Nets 11. Practical Methodology 12. Applications Part III. Deep Learning Research 13. Linear Factor Models 14. Autoencoders 15. Representation Learning 16. Structured Probabilistic Models for Deep Learning 17. Monte Carlo Methods 18. Confronting the Partition Function 19. Approximate Inference 20. Deep Generative Models","title":"Contents"},{"location":"01/","text":"01. Introduction \u00b6 Contents \u00b6 01.01. Who Should Read This Book? 01.02. Historical Trends in Deep Learning 01.02.01. The Many Names and Changing Fortunes of Neural Networks 01.02.02. Increasing Dataset Sizes 01.02.03. INcreasing Model Sizes 01.02.04. Increasing Accuracy, Complexity and Real-World Impact \u00b6 ancient Greece Galatea by Pygmalion Talos by Daedalus Pandora by Hephaestus AI deep learning logistic regression naive Bayes 01.01. Who Should Read This Book? \u00b6 01.02. Historical Trends in Deep Learning \u00b6 01.02.01. The Many Names and Changing Fortunes of Neural Networks \u00b6 cybernetics in 1940s-1960s connectionism in 1980s-1990s deep learning in 2006 artificial neural networks (ANNs) f(\\mathbf{x}, \\mathbf{w}) = x_1 x_1 + \\cdots + x_n w_n McCulloch-Pitts Neuron ( McCulloch, Pitts, 1943 ) perceptron ( Rosenblatt, 1958 , 1962 ) ADALINE ( Widrow, Hoff, 1960 ) stochasic gradient descent linear models limitation: connot learn the XOR fn \\begin{cases} f([0,1], \\mathbf{w}) = 1 \\\\ f([1,0], \\mathbf{w}) = 1 \\\\ f([1,1], \\mathbf{w}) = 1 \\\\ f([0,0], \\mathbf{w}) = 0 \\end{cases} Minsky, Papert, 1969 Olshausen, Field, 2005 ferrets brain ( Von Melshner, 2000 ) Neocognitron ( Fukushima, 1980 ) \u2192 modern convolutional network ( LeCun, 1998 ) rectified linear unit original Cognitron ( Fukushima, 1975 ) Nair, Hinton, 2010 , Glorot, 2011 citing neuroscience, Jarrett, 2009 citing more engineering-oriented 1980s: second wave of NN movement connectionism / parallel distributed processing ( Rumelhart, 1986 , McClelland, 1995 ) models of cognition grounded in neural implementation ( Touretzky, Minton, 1985 ), reviving work og Fonald Hebb in 1940s ( Hebb, 1949 ) connectionsim: large num of sumple computational units can ahchive intelligent behavior distributed representation ( Hinton, 1986 ) e.g., 3 neurons describing 3 colors & 3 neurons 3 objects back-propagation ( Rumelhart, 1986 , LeCun, 1987 ) during 1990s Hochreiter, 1991 , Bengio, 1994 Long short-term memody (LSTM) ( Hochreiter, Schmidhuber, 1997 ) mid-1990s Kernel machines ( Boser, 1992 , Corters, Vapnik, 1995 , Sch\u00f6lkopf, 1999 ) graphical models ( Jordan, 1998 ) LeCun, 1998 , Bengio, 2001 3rd wave of NN deep belief network trained using greedy layer-wise pretraining ( Hinton, 2006 ) Bengio, 2007 , Ranzato, 2007 01.02.02. Increasing Dataset Sizes \u00b6 01.02.03. Increasing Model Sizes \u00b6 01.02.04. Increasing Accuracy, Complexity and Real-World Impact \u00b6 \u00b6 img{width: 51%; float: right;}","title":"01. Introduction"},{"location":"01/#contents","text":"01.01. Who Should Read This Book? 01.02. Historical Trends in Deep Learning 01.02.01. The Many Names and Changing Fortunes of Neural Networks 01.02.02. Increasing Dataset Sizes 01.02.03. INcreasing Model Sizes 01.02.04. Increasing Accuracy, Complexity and Real-World Impact","title":"Contents"},{"location":"01/#0101_who_should_read_this_book","text":"","title":"01.01. Who Should Read This Book?"},{"location":"01/#0102_historical_trends_in_deep_learning","text":"","title":"01.02. Historical Trends in Deep Learning"},{"location":"01/#010201_the_many_names_and_changing_fortunes_of_neural_networks","text":"cybernetics in 1940s-1960s connectionism in 1980s-1990s deep learning in 2006 artificial neural networks (ANNs) f(\\mathbf{x}, \\mathbf{w}) = x_1 x_1 + \\cdots + x_n w_n McCulloch-Pitts Neuron ( McCulloch, Pitts, 1943 ) perceptron ( Rosenblatt, 1958 , 1962 ) ADALINE ( Widrow, Hoff, 1960 ) stochasic gradient descent linear models limitation: connot learn the XOR fn \\begin{cases} f([0,1], \\mathbf{w}) = 1 \\\\ f([1,0], \\mathbf{w}) = 1 \\\\ f([1,1], \\mathbf{w}) = 1 \\\\ f([0,0], \\mathbf{w}) = 0 \\end{cases} Minsky, Papert, 1969 Olshausen, Field, 2005 ferrets brain ( Von Melshner, 2000 ) Neocognitron ( Fukushima, 1980 ) \u2192 modern convolutional network ( LeCun, 1998 ) rectified linear unit original Cognitron ( Fukushima, 1975 ) Nair, Hinton, 2010 , Glorot, 2011 citing neuroscience, Jarrett, 2009 citing more engineering-oriented 1980s: second wave of NN movement connectionism / parallel distributed processing ( Rumelhart, 1986 , McClelland, 1995 ) models of cognition grounded in neural implementation ( Touretzky, Minton, 1985 ), reviving work og Fonald Hebb in 1940s ( Hebb, 1949 ) connectionsim: large num of sumple computational units can ahchive intelligent behavior distributed representation ( Hinton, 1986 ) e.g., 3 neurons describing 3 colors & 3 neurons 3 objects back-propagation ( Rumelhart, 1986 , LeCun, 1987 ) during 1990s Hochreiter, 1991 , Bengio, 1994 Long short-term memody (LSTM) ( Hochreiter, Schmidhuber, 1997 ) mid-1990s Kernel machines ( Boser, 1992 , Corters, Vapnik, 1995 , Sch\u00f6lkopf, 1999 ) graphical models ( Jordan, 1998 ) LeCun, 1998 , Bengio, 2001 3rd wave of NN deep belief network trained using greedy layer-wise pretraining ( Hinton, 2006 ) Bengio, 2007 , Ranzato, 2007","title":"01.02.01. The Many Names and Changing Fortunes of Neural Networks"},{"location":"01/#010202_increasing_dataset_sizes","text":"","title":"01.02.02. Increasing Dataset Sizes"},{"location":"01/#010203_increasing_model_sizes","text":"","title":"01.02.03. Increasing Model Sizes"},{"location":"01/#010204_increasing_accuracy_complexity_and_real-world_impact","text":"","title":"01.02.04. Increasing Accuracy, Complexity and Real-World Impact"},{"location":"02/","text":"02. Linear Algebra \u00b6 Contents \u00b6 02.01. Scalars, Vectors, Matrices and Tensors 02.02. Multiplying Matrices and Vectors 02.03. Identity and Inverse Matrices 02.01. Scalars, Vectors, Matrices and Tensors \u00b6 scalars vectors matrices: \\mathbf{A} tensors: \\mathsf{A}_{i, j, k} 02.02. Multiplying Matrices and Vectors \u00b6 02.03. Identity and Inverse Matrices \u00b6 \u00b6 img{width: 51%; float: right;}","title":"02. Linear Algebra"},{"location":"02/#contents","text":"02.01. Scalars, Vectors, Matrices and Tensors 02.02. Multiplying Matrices and Vectors 02.03. Identity and Inverse Matrices","title":"Contents"},{"location":"02/#0201_scalars_vectors_matrices_and_tensors","text":"scalars vectors matrices: \\mathbf{A} tensors: \\mathsf{A}_{i, j, k}","title":"02.01. Scalars, Vectors, Matrices and Tensors"},{"location":"02/#0202_multiplying_matrices_and_vectors","text":"","title":"02.02. Multiplying Matrices and Vectors"},{"location":"02/#0203_identity_and_inverse_matrices","text":"","title":"02.03. Identity and Inverse Matrices"}]}
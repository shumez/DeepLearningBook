{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Deep Learning Book \u00b6 Contents \u00b6 Part 0. 01. Introduction Part I. Applied Math and Machine Learning Basics 02. Linear Algebra 03. Probability and Information Theory 04. Numerical Computation 05. Machine Learning Basics Part II. Deep Networks: Modern Practices 06. Deep Feedforward Networks 07. Regularization for Deep Learning 08. Optimization for Training Deep Models 09. Convolutional Networks 10. Sequence Modeling: Recurrent and Recursive Nets 11. Practical Methodology 12. Applications Part III. Deep Learning Research 13. Linear Factor Models 14. Autoencoders 15. Representation Learning 16. Structured Probabilistic Models for Deep Learning 17. Monte Carlo Methods 18. Confronting the Partition Function 19. Approximate Inference 20. Deep Generative Models Resources \u00b6 Deep Learning Book Lecture github \u00b6 img{width: 51%; float: right;}","title":"Home"},{"location":"#contents","text":"Part 0. 01. Introduction Part I. Applied Math and Machine Learning Basics 02. Linear Algebra 03. Probability and Information Theory 04. Numerical Computation 05. Machine Learning Basics Part II. Deep Networks: Modern Practices 06. Deep Feedforward Networks 07. Regularization for Deep Learning 08. Optimization for Training Deep Models 09. Convolutional Networks 10. Sequence Modeling: Recurrent and Recursive Nets 11. Practical Methodology 12. Applications Part III. Deep Learning Research 13. Linear Factor Models 14. Autoencoders 15. Representation Learning 16. Structured Probabilistic Models for Deep Learning 17. Monte Carlo Methods 18. Confronting the Partition Function 19. Approximate Inference 20. Deep Generative Models","title":"Contents"},{"location":"#resources","text":"Deep Learning Book Lecture github","title":"Resources"},{"location":"01/","text":"01. Introduction \u00b6 Contents \u00b6 01.00. 01.01. Who Should Read This Book? 01.02. Historical Trends in Deep Learning 01.02.01. The Many Names and Changing Fortunes of Neural Networks 01.02.02. Increasing Dataset Sizes 01.02.03. INcreasing Model Sizes 01.02.04. Increasing Accuracy, Complexity and Real-World Impact 01.00 \u00b6 ancient Greece Galatea by Pygmalion Talos by Daedalus Pandora by Hephaestus AI deep learning logistic regression naive Bayes 01.01. Who Should Read This Book? \u00b6 01.02. Historical Trends in Deep Learning \u00b6 01.02.01. The Many Names and Changing Fortunes of Neural Networks \u00b6 cybernetics in 1940s-1960s connectionism in 1980s-1990s deep learning in 2006 artificial neural networks (ANNs) f(\\mathbf{x}, \\mathbf{w}) = x_1 x_1 + \\cdots + x_n w_n McCulloch-Pitts Neuron ( McCulloch, Pitts, 1943 ) perceptron ( Rosenblatt, 1958 , 1962 ) ADALINE ( Widrow, Hoff, 1960 ) stochasic gradient descent linear models limitation: connot learn the XOR fn \\begin{cases} f([0,1], \\mathbf{w}) = 1 \\\\ f([1,0], \\mathbf{w}) = 1 \\\\ f([1,1], \\mathbf{w}) = 1 \\\\ f([0,0], \\mathbf{w}) = 0 \\end{cases} Minsky, Papert, 1969 Olshausen, Field, 2005 ferrets brain ( Von Melshner, 2000 ) Neocognitron ( Fukushima, 1980 ) \u2192 modern convolutional network ( LeCun, 1998 ) rectified linear unit original Cognitron ( Fukushima, 1975 ) Nair, Hinton, 2010 , Glorot, 2011 citing neuroscience, Jarrett, 2009 citing more engineering-oriented 1980s: second wave of NN movement connectionism / parallel distributed processing ( Rumelhart, 1986 , McClelland, 1995 ) models of cognition grounded in neural implementation ( Touretzky, Minton, 1985 ), reviving work og Fonald Hebb in 1940s ( Hebb, 1949 ) connectionsim: large num of sumple computational units can ahchive intelligent behavior distributed representation ( Hinton, 1986 ) e.g., 3 neurons describing 3 colors & 3 neurons 3 objects back-propagation ( Rumelhart, 1986 , LeCun, 1987 ) during 1990s Hochreiter, 1991 , Bengio, 1994 Long short-term memody (LSTM) ( Hochreiter, Schmidhuber, 1997 ) mid-1990s Kernel machines ( Boser, 1992 , Corters, Vapnik, 1995 , Sch\u00f6lkopf, 1999 ) graphical models ( Jordan, 1998 ) LeCun, 1998 , Bengio, 2001 3rd wave of NN deep belief network trained using greedy layer-wise pretraining ( Hinton, 2006 ) Bengio, 2007 , Ranzato, 2007 01.02.02. Increasing Dataset Sizes \u00b6 01.02.03. Increasing Model Sizes \u00b6 01.02.04. Increasing Accuracy, Complexity and Real-World Impact \u00b6 \u00b6 img{width: 51%; float: right;}","title":"01. Introduction"},{"location":"01/#contents","text":"01.00. 01.01. Who Should Read This Book? 01.02. Historical Trends in Deep Learning 01.02.01. The Many Names and Changing Fortunes of Neural Networks 01.02.02. Increasing Dataset Sizes 01.02.03. INcreasing Model Sizes 01.02.04. Increasing Accuracy, Complexity and Real-World Impact","title":"Contents"},{"location":"01/#0100","text":"ancient Greece Galatea by Pygmalion Talos by Daedalus Pandora by Hephaestus AI deep learning logistic regression naive Bayes","title":"01.00"},{"location":"01/#0101_who_should_read_this_book","text":"","title":"01.01. Who Should Read This Book?"},{"location":"01/#0102_historical_trends_in_deep_learning","text":"","title":"01.02. Historical Trends in Deep Learning"},{"location":"01/#010201_the_many_names_and_changing_fortunes_of_neural_networks","text":"cybernetics in 1940s-1960s connectionism in 1980s-1990s deep learning in 2006 artificial neural networks (ANNs) f(\\mathbf{x}, \\mathbf{w}) = x_1 x_1 + \\cdots + x_n w_n McCulloch-Pitts Neuron ( McCulloch, Pitts, 1943 ) perceptron ( Rosenblatt, 1958 , 1962 ) ADALINE ( Widrow, Hoff, 1960 ) stochasic gradient descent linear models limitation: connot learn the XOR fn \\begin{cases} f([0,1], \\mathbf{w}) = 1 \\\\ f([1,0], \\mathbf{w}) = 1 \\\\ f([1,1], \\mathbf{w}) = 1 \\\\ f([0,0], \\mathbf{w}) = 0 \\end{cases} Minsky, Papert, 1969 Olshausen, Field, 2005 ferrets brain ( Von Melshner, 2000 ) Neocognitron ( Fukushima, 1980 ) \u2192 modern convolutional network ( LeCun, 1998 ) rectified linear unit original Cognitron ( Fukushima, 1975 ) Nair, Hinton, 2010 , Glorot, 2011 citing neuroscience, Jarrett, 2009 citing more engineering-oriented 1980s: second wave of NN movement connectionism / parallel distributed processing ( Rumelhart, 1986 , McClelland, 1995 ) models of cognition grounded in neural implementation ( Touretzky, Minton, 1985 ), reviving work og Fonald Hebb in 1940s ( Hebb, 1949 ) connectionsim: large num of sumple computational units can ahchive intelligent behavior distributed representation ( Hinton, 1986 ) e.g., 3 neurons describing 3 colors & 3 neurons 3 objects back-propagation ( Rumelhart, 1986 , LeCun, 1987 ) during 1990s Hochreiter, 1991 , Bengio, 1994 Long short-term memody (LSTM) ( Hochreiter, Schmidhuber, 1997 ) mid-1990s Kernel machines ( Boser, 1992 , Corters, Vapnik, 1995 , Sch\u00f6lkopf, 1999 ) graphical models ( Jordan, 1998 ) LeCun, 1998 , Bengio, 2001 3rd wave of NN deep belief network trained using greedy layer-wise pretraining ( Hinton, 2006 ) Bengio, 2007 , Ranzato, 2007","title":"01.02.01. The Many Names and Changing Fortunes of Neural Networks"},{"location":"01/#010202_increasing_dataset_sizes","text":"","title":"01.02.02. Increasing Dataset Sizes"},{"location":"01/#010203_increasing_model_sizes","text":"","title":"01.02.03. Increasing Model Sizes"},{"location":"01/#010204_increasing_accuracy_complexity_and_real-world_impact","text":"","title":"01.02.04. Increasing Accuracy, Complexity and Real-World Impact"},{"location":"02/","text":"02. Linear Algebra \u00b6 Contents \u00b6 02.00. 02.01. Scalars, Vectors, Matrices and Tensors 02.02. Multiplying Matrices and Vectors 02.03. Identity and Inverse Matrices 02.04. Linear Dependence and Span 02.05. Norms 02.06. Special Kinds of Matricesand Vectors 02.07. Eigendecomposition 02.08. Singular Value Decomposition 02.09. The Moore-Penrose Pseudoinverse 02.10. The Trace Operator 02.11. The Determinant 02.12. Example: Principal Components Analysis 02.00. \u00b6 The Matrix Cookbook ( Peterson, Pedersen, 2006 ) Shilov, 1977 02.01. Scalars, Vectors, Matrices and Tensors \u00b6 scalars vectors: \\mathbf{x} \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\tag{2.1} matrices: \\mathbf{A} \\mathbf{A} = \\begin{bmatrix} A_{1,1} & A_{1,2} \\\\ A_{2,1} & A_{2,2} \\\\ A_{3,1} & A_{3,2} \\end{bmatrix} \\Rightarrow \\mathbf{A}^T = \\begin{bmatrix} A_{1,1} & A_{2,1} & A_{3,1} \\\\ A_{1,2} & A_{2,2} & A_{3,2} \\end{bmatrix} tensors: \\mathsf{A} A_{i, j, k} def transposition (\\mathbf{A}^T)_{i,j} = \\mathbf{A}_{j,i} \\tag{2.3} \\mathbf{C} = \\mathbf{A} + \\mathbf{B} where C_{i,j} = A_{i,j} + B_{i,j} \\mathbf{D} = a \\cdot \\mathbf{B} + c where D_{i,j} = a \\cdot B_{i,j} + c \\mathbf{C} = \\mathbf{A} + \\mathbf{b} where C_{i,j} = A_{i,j} + b_j broadacsting 02.02. Multiplying Matrices and Vectors \u00b6 matrix product \\mathbf{C} = \\mathbf{AB} \\tag{2.4} C_{i,j} = \\sum_k{A_{i,k} B_{k,j}} \\tag{2.5} element-wise product (aka, Hadamard product ) \\mathbf{C} = \\mathbf{A} \\odot \\mathbf{B} C_{i,j} = A_{i,j} B_{i,j} dot product : \\mathbf{x} \\cdot \\mathbf{y} = \\mathbf{x}^T \\mathbf{y} \\mathbf{x}^T \\mathbf{y} = \\mathbf{y}^T \\mathbf{x} \\tag{2.8} (\\mathbf{AB})^T = \\mathbf{B}^T \\mathbf{A}^T \\tag{2.9} \\mathbf{x}^T \\mathbf{y} = (\\mathbf{x}^T \\mathbf{y})^T = \\mathbf{y}^T \\mathbf{x} \\tag{2.10} a system of linear equations: \\mathbf{Ax} = \\mathbf{b} \\tag{2.11} where \\mathbf{A} \\in \\mathbb{R}^{m \\times n} , \\mathbf{b} \\in \\mathbb{R}^m , \\mathbf{x} \\in \\mathbb{R}^n \\begin{align*} \\mathbf{A}_{1,:} \\mathbf{x} &= b_1 \\tag{2.12} \\\\ \\mathbf{A}_{2,:} \\mathbf{x} &= b_2 \\tag{2.13} \\\\ \\vdots \\tag{2.14} \\\\ \\mathbf{A}_{m,:} \\mathbf{x} &= b_m \\tag{2.15} \\\\ \\end{align*} \\begin{align*} \\mathbf{A}_{1,1} x_1 + \\mathbf{A}_{1,2} x_2 + \\cdots + \\mathbf{A}_{1,n} x_n &= b_1 \\tag{2.16} \\\\ \\mathbf{A}_{2,1} x_1 + \\mathbf{A}_{2,2} x_2 + \\cdots + \\mathbf{A}_{2,n} x_n &= b_2 \\tag{2.17} \\\\ \\vdots \\tag{2.18} \\\\ \\mathbf{A}_{m,1} x_1 + \\mathbf{A}_{m,2} x_2 + \\cdots + \\mathbf{A}_{m,n} x_n &= b_m \\tag{2.19} \\end{align*} 02.03. Identity and Inverse Matrices \u00b6 matrix inversion identity matrix \\mathbf{I}_n \\in \\mathbb{R}^{n \\times n} \\forall\\mathbf{x} \\in \\mathbb{R}^n, \\mathbf{I}_n \\mathbf{x} = \\mathbf{x} \\tag{2.20} \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_n \\tag{2.21} 02.04. Linear Dependence and Span \u00b6 in order for \\mathbf{A}^{-1} to exist \\mathbf{Ax} = \\mathbf{b} \\tag{2.11} eq.2.11 must have one solution possible to have no solution / infinitely many solutions NOT possible to have more than 1 but less than infinitely solutions if both \\mathbf{x} and \\mathbf{y} are solutions then \\mathbf{z} = \\alpha \\mathbf{x} + (1 - \\alpha) \\mathbf{y} \\tag{2.26} is also solution for any real \\alpha \\mathbf{Ax} = \\sum_i{x_i \\mathbf{A}_{:,i}} \\tag{2.27} linear combination \\{ \\mathbf{v}^{(1)}, \\cdots, \\mathbf{v}^{(n)} \\} \\sum_i{c_i \\mathbf{v}^{(i)}} \\tag{2.28} span \\mathbf{Ax} = \\mathbf{b} have solution for all vals of \\mathbf{b} \\in \\mathbb{R}^m 02.05. Norms \u00b6 norm : fn that measure size of vectors L^p norm is given by ||x||_p = \\Bigg( \\sum_i{|x_i|^p} \\Bigg)^{\\frac{1}{p}} \\tag{2.30} for p \\in \\mathbb{R} , p \\ge 1 f(\\mathbf{x}) = 0 \u21d2 x=0 f(\\mathbf{x} + \\mathbf{y}) \\le f(\\mathbf{x}) + f(\\mathbf{y}) ( triangle inequality ) \\forall\\alpha \\in \\mathbb{R} , f(\\alpha \\mathbf{x}) = |\\alpha| f(x) L^2 norm, with p=2 ka Euclidean norm ( ||x|| ) L^1 norm ||x||_1 = \\sum_i{|x_i|} \\tag{2.31} L^\\infty norm (aka, max norm ) ||x||_\\infty = \\max_i{|x_i|} \\tag{2.32} Frobenius norm ||A||_F = \\sqrt{\\sum_{i,j}{\\mathbf{A}_{i,j}^2}} \\tag{2.33} is analogous to L^2 norm of vec \\mathbf{x}^T \\mathbf{y} = ||\\mathbf{x}||_2 ||\\mathbf{y}||_2 \\cos{\\theta} \\tag{2.34} where \\theta is angle between \\mathbf{x} , \\mathbf{y} 02.06. Special Kinds of Matricesand Vectors \u00b6 diagonal \\mathbf{D} if and only if D_{i,j}=0 for all i \\ne j np.random.seed(123) v = np.random.randint(9, size=5) np.diag(v) array([[2, 0, 0, 0, 0], [0, 2, 0, 0, 0], [0, 0, 6, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 3]]) \\text{diag}(\\mathbf{v}) \\mathbf{x} = \\mathbf{v} \\odot \\mathbf{x} np.matmul(np.diag(v), x) np.multiply(v, x) \\text{diag}(\\mathbf{v})^{-1} = \\text{diag}\\Bigg( \\bigg[\\frac{1}{v_1}, \\cdots \\frac{1}{v_n} \\bigg]^T \\Bigg) np.linalg.inv(diag_v) np.diag([1/vi for vi in v]) symmetric mat \\mathbf{A} = \\mathbf{A}^T \\tag{2.35} unit vector : vec w/ unit norm : ||x||_2 = 1 \\tag{2.36} vec \\mathbf{x} and vec \\mathbf{y} orthogonal to each other if \\mathbf{x}^T \\mathbf{y} = 0 orthogonal & unit norm orthonormal orthogonal matrix \\mathbf{A}^T \\mathbf{A} = \\mathbf{A}\\mathbf{A}^T = \\mathbf{I} \\tag{2.37} implyies \\mathbf{A}^{-1} = \\mathbf{A}^T \\tag{2.38} from scipy.stats import ortho_group A = ortho_group.rvs(dim = 3) np.set_printoptions(suppress=True) AT_A = np.matmul(A, A.T) 02.07. Eigendecomposition \u00b6 eigendecomposition : into set of eigenvectors & eigenvalues eigenvector of square mat \\mathbf{A} is non-zero vec \\mathbf{v} \\mathbf{A}\\mathbf{v} = \\lambda \\mathbf{v} \\tag{2.39} eigenvalue \\lambda l, V = np.linalg.eig(A) mat \\mathbf{A} has n linearly independent eigenvec \\{ \\mathbf{v}^{(1)}, \\cdots, \\mathbf{v}^{(n)} \\} , w/ corresponding eigenval \\{ \\lambda_1, \\cdots, \\lambda_n \\} mat \\mathbf{V} : \\mathbf{V} = [ \\mathbf{v}^{(1)}, \\cdots, \\mathbf{v}^{(b)} ] , vec \\lambda : [\\lambda_1, \\cdots, \\lambda_n]^T eigendecomposition \\mathbf{A} = \\mathbf{V} \\text{diag}(\\lambda) \\mathbf{V}^{-1} \\tag{2.40} np.linalg.multi_dot([V, np.diag(l), np.linalg.inv(V)]) \\begin{align*} \\mathbf{A} &= \\mathbf{Q \\Lambda Q}^T \\\\ &= \\begin{bmatrix} \\mathbf{v}_1 & \\mathbf{v}_2 & \\cdots & \\mathbf{v}_n \\end{bmatrix} \\begin{bmatrix} \\lambda_1 & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\lambda_n \\end{bmatrix} \\begin{bmatrix} \\mathbf{v}_1 \\\\ \\mathbf{v}_2 \\\\ \\vdots \\\\ \\mathbf{v}_n \\end{bmatrix} \\end{align*} \\tag{2.41} orthogonal mat \\mathbf{Q} , eigenvectors \\mathbf{A} , diagonal mat \\mathbf{\\Lambda} eigenvalue \\mathbf{\\Lambda}_{i,i} is associated w/ eigenvector \\mathbf{Q}_{:,i} 02.08. Singular Value Decomposition \u00b6 02.09. The Moore-Penrose Pseudoinverse \u00b6 02.10. The Trace Operator \u00b6 02.11. The Determinant \u00b6 02.12. Example: Principal Components Analysis \u00b6 \u00b6 img{width: 51%; float: right;}","title":"02. Linear Algebra"},{"location":"02/#contents","text":"02.00. 02.01. Scalars, Vectors, Matrices and Tensors 02.02. Multiplying Matrices and Vectors 02.03. Identity and Inverse Matrices 02.04. Linear Dependence and Span 02.05. Norms 02.06. Special Kinds of Matricesand Vectors 02.07. Eigendecomposition 02.08. Singular Value Decomposition 02.09. The Moore-Penrose Pseudoinverse 02.10. The Trace Operator 02.11. The Determinant 02.12. Example: Principal Components Analysis","title":"Contents"},{"location":"02/#0200","text":"The Matrix Cookbook ( Peterson, Pedersen, 2006 ) Shilov, 1977","title":"02.00."},{"location":"02/#0201_scalars_vectors_matrices_and_tensors","text":"scalars vectors: \\mathbf{x} \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\tag{2.1} matrices: \\mathbf{A} \\mathbf{A} = \\begin{bmatrix} A_{1,1} & A_{1,2} \\\\ A_{2,1} & A_{2,2} \\\\ A_{3,1} & A_{3,2} \\end{bmatrix} \\Rightarrow \\mathbf{A}^T = \\begin{bmatrix} A_{1,1} & A_{2,1} & A_{3,1} \\\\ A_{1,2} & A_{2,2} & A_{3,2} \\end{bmatrix} tensors: \\mathsf{A} A_{i, j, k} def transposition (\\mathbf{A}^T)_{i,j} = \\mathbf{A}_{j,i} \\tag{2.3} \\mathbf{C} = \\mathbf{A} + \\mathbf{B} where C_{i,j} = A_{i,j} + B_{i,j} \\mathbf{D} = a \\cdot \\mathbf{B} + c where D_{i,j} = a \\cdot B_{i,j} + c \\mathbf{C} = \\mathbf{A} + \\mathbf{b} where C_{i,j} = A_{i,j} + b_j broadacsting","title":"02.01. Scalars, Vectors, Matrices and Tensors"},{"location":"02/#0202_multiplying_matrices_and_vectors","text":"matrix product \\mathbf{C} = \\mathbf{AB} \\tag{2.4} C_{i,j} = \\sum_k{A_{i,k} B_{k,j}} \\tag{2.5} element-wise product (aka, Hadamard product ) \\mathbf{C} = \\mathbf{A} \\odot \\mathbf{B} C_{i,j} = A_{i,j} B_{i,j} dot product : \\mathbf{x} \\cdot \\mathbf{y} = \\mathbf{x}^T \\mathbf{y} \\mathbf{x}^T \\mathbf{y} = \\mathbf{y}^T \\mathbf{x} \\tag{2.8} (\\mathbf{AB})^T = \\mathbf{B}^T \\mathbf{A}^T \\tag{2.9} \\mathbf{x}^T \\mathbf{y} = (\\mathbf{x}^T \\mathbf{y})^T = \\mathbf{y}^T \\mathbf{x} \\tag{2.10} a system of linear equations: \\mathbf{Ax} = \\mathbf{b} \\tag{2.11} where \\mathbf{A} \\in \\mathbb{R}^{m \\times n} , \\mathbf{b} \\in \\mathbb{R}^m , \\mathbf{x} \\in \\mathbb{R}^n \\begin{align*} \\mathbf{A}_{1,:} \\mathbf{x} &= b_1 \\tag{2.12} \\\\ \\mathbf{A}_{2,:} \\mathbf{x} &= b_2 \\tag{2.13} \\\\ \\vdots \\tag{2.14} \\\\ \\mathbf{A}_{m,:} \\mathbf{x} &= b_m \\tag{2.15} \\\\ \\end{align*} \\begin{align*} \\mathbf{A}_{1,1} x_1 + \\mathbf{A}_{1,2} x_2 + \\cdots + \\mathbf{A}_{1,n} x_n &= b_1 \\tag{2.16} \\\\ \\mathbf{A}_{2,1} x_1 + \\mathbf{A}_{2,2} x_2 + \\cdots + \\mathbf{A}_{2,n} x_n &= b_2 \\tag{2.17} \\\\ \\vdots \\tag{2.18} \\\\ \\mathbf{A}_{m,1} x_1 + \\mathbf{A}_{m,2} x_2 + \\cdots + \\mathbf{A}_{m,n} x_n &= b_m \\tag{2.19} \\end{align*}","title":"02.02. Multiplying Matrices and Vectors"},{"location":"02/#0203_identity_and_inverse_matrices","text":"matrix inversion identity matrix \\mathbf{I}_n \\in \\mathbb{R}^{n \\times n} \\forall\\mathbf{x} \\in \\mathbb{R}^n, \\mathbf{I}_n \\mathbf{x} = \\mathbf{x} \\tag{2.20} \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_n \\tag{2.21}","title":"02.03. Identity and Inverse Matrices"},{"location":"02/#0204_linear_dependence_and_span","text":"in order for \\mathbf{A}^{-1} to exist \\mathbf{Ax} = \\mathbf{b} \\tag{2.11} eq.2.11 must have one solution possible to have no solution / infinitely many solutions NOT possible to have more than 1 but less than infinitely solutions if both \\mathbf{x} and \\mathbf{y} are solutions then \\mathbf{z} = \\alpha \\mathbf{x} + (1 - \\alpha) \\mathbf{y} \\tag{2.26} is also solution for any real \\alpha \\mathbf{Ax} = \\sum_i{x_i \\mathbf{A}_{:,i}} \\tag{2.27} linear combination \\{ \\mathbf{v}^{(1)}, \\cdots, \\mathbf{v}^{(n)} \\} \\sum_i{c_i \\mathbf{v}^{(i)}} \\tag{2.28} span \\mathbf{Ax} = \\mathbf{b} have solution for all vals of \\mathbf{b} \\in \\mathbb{R}^m","title":"02.04. Linear Dependence and Span"},{"location":"02/#0205_norms","text":"norm : fn that measure size of vectors L^p norm is given by ||x||_p = \\Bigg( \\sum_i{|x_i|^p} \\Bigg)^{\\frac{1}{p}} \\tag{2.30} for p \\in \\mathbb{R} , p \\ge 1 f(\\mathbf{x}) = 0 \u21d2 x=0 f(\\mathbf{x} + \\mathbf{y}) \\le f(\\mathbf{x}) + f(\\mathbf{y}) ( triangle inequality ) \\forall\\alpha \\in \\mathbb{R} , f(\\alpha \\mathbf{x}) = |\\alpha| f(x) L^2 norm, with p=2 ka Euclidean norm ( ||x|| ) L^1 norm ||x||_1 = \\sum_i{|x_i|} \\tag{2.31} L^\\infty norm (aka, max norm ) ||x||_\\infty = \\max_i{|x_i|} \\tag{2.32} Frobenius norm ||A||_F = \\sqrt{\\sum_{i,j}{\\mathbf{A}_{i,j}^2}} \\tag{2.33} is analogous to L^2 norm of vec \\mathbf{x}^T \\mathbf{y} = ||\\mathbf{x}||_2 ||\\mathbf{y}||_2 \\cos{\\theta} \\tag{2.34} where \\theta is angle between \\mathbf{x} , \\mathbf{y}","title":"02.05. Norms"},{"location":"02/#0206_special_kinds_of_matricesand_vectors","text":"diagonal \\mathbf{D} if and only if D_{i,j}=0 for all i \\ne j np.random.seed(123) v = np.random.randint(9, size=5) np.diag(v) array([[2, 0, 0, 0, 0], [0, 2, 0, 0, 0], [0, 0, 6, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 3]]) \\text{diag}(\\mathbf{v}) \\mathbf{x} = \\mathbf{v} \\odot \\mathbf{x} np.matmul(np.diag(v), x) np.multiply(v, x) \\text{diag}(\\mathbf{v})^{-1} = \\text{diag}\\Bigg( \\bigg[\\frac{1}{v_1}, \\cdots \\frac{1}{v_n} \\bigg]^T \\Bigg) np.linalg.inv(diag_v) np.diag([1/vi for vi in v]) symmetric mat \\mathbf{A} = \\mathbf{A}^T \\tag{2.35} unit vector : vec w/ unit norm : ||x||_2 = 1 \\tag{2.36} vec \\mathbf{x} and vec \\mathbf{y} orthogonal to each other if \\mathbf{x}^T \\mathbf{y} = 0 orthogonal & unit norm orthonormal orthogonal matrix \\mathbf{A}^T \\mathbf{A} = \\mathbf{A}\\mathbf{A}^T = \\mathbf{I} \\tag{2.37} implyies \\mathbf{A}^{-1} = \\mathbf{A}^T \\tag{2.38} from scipy.stats import ortho_group A = ortho_group.rvs(dim = 3) np.set_printoptions(suppress=True) AT_A = np.matmul(A, A.T)","title":"02.06. Special Kinds of Matricesand Vectors"},{"location":"02/#0207_eigendecomposition","text":"eigendecomposition : into set of eigenvectors & eigenvalues eigenvector of square mat \\mathbf{A} is non-zero vec \\mathbf{v} \\mathbf{A}\\mathbf{v} = \\lambda \\mathbf{v} \\tag{2.39} eigenvalue \\lambda l, V = np.linalg.eig(A) mat \\mathbf{A} has n linearly independent eigenvec \\{ \\mathbf{v}^{(1)}, \\cdots, \\mathbf{v}^{(n)} \\} , w/ corresponding eigenval \\{ \\lambda_1, \\cdots, \\lambda_n \\} mat \\mathbf{V} : \\mathbf{V} = [ \\mathbf{v}^{(1)}, \\cdots, \\mathbf{v}^{(b)} ] , vec \\lambda : [\\lambda_1, \\cdots, \\lambda_n]^T eigendecomposition \\mathbf{A} = \\mathbf{V} \\text{diag}(\\lambda) \\mathbf{V}^{-1} \\tag{2.40} np.linalg.multi_dot([V, np.diag(l), np.linalg.inv(V)]) \\begin{align*} \\mathbf{A} &= \\mathbf{Q \\Lambda Q}^T \\\\ &= \\begin{bmatrix} \\mathbf{v}_1 & \\mathbf{v}_2 & \\cdots & \\mathbf{v}_n \\end{bmatrix} \\begin{bmatrix} \\lambda_1 & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\lambda_n \\end{bmatrix} \\begin{bmatrix} \\mathbf{v}_1 \\\\ \\mathbf{v}_2 \\\\ \\vdots \\\\ \\mathbf{v}_n \\end{bmatrix} \\end{align*} \\tag{2.41} orthogonal mat \\mathbf{Q} , eigenvectors \\mathbf{A} , diagonal mat \\mathbf{\\Lambda} eigenvalue \\mathbf{\\Lambda}_{i,i} is associated w/ eigenvector \\mathbf{Q}_{:,i}","title":"02.07. Eigendecomposition"},{"location":"02/#0208_singular_value_decomposition","text":"","title":"02.08. Singular Value Decomposition"},{"location":"02/#0209_the_moore-penrose_pseudoinverse","text":"","title":"02.09. The Moore-Penrose Pseudoinverse"},{"location":"02/#0210_the_trace_operator","text":"","title":"02.10. The Trace Operator"},{"location":"02/#0211_the_determinant","text":"","title":"02.11. The Determinant"},{"location":"02/#0212_example_principal_components_analysis","text":"","title":"02.12. Example: Principal Components Analysis"}]}